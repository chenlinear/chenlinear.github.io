<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://chenlinear.github.io"><title>寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上 | Chen Li</title><meta name=description content="Chen Li's personal blog"><meta property="og:title" content="寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上"><meta property="og:description" content="还未经测试!
显卡：寒武纪 MLU370-X4。
显卡坞的解决方式恐怕不行，因为如果重新更换设备需要重新从 ubuntu 开始装起，不如在另一台机器上构建一个本地服务器。除此之外的问题还有：笔记本自带的核显会打架吗？硬盘容量不够了。:(
视频介绍（除3.几乎完全是这个知乎专栏的内容）：
寒武纪 MLU370 加速卡简介及安装 寒武纪基础软件平台安装 MLU370 开发实战 文档（在熟悉视频介绍之后再查看）：
文档中心 MLU370-X4 智能加速卡产品手册 1.0.0 文档 我设想的本地服务器的构建流程：
安装 ubuntu，版本号见视频介绍：16.04或18.04? 按照视频介绍安装 torch.mlu 组件等等（和 Dive into Deep Learning1 这本书不同的地方在于，寒武纪在使用 pytorch 时导入 torch 和 torch.mlu 即可，不需安装 torchvision 即 pytorch 的 GPU 版本，此处 mlu 和 gpu 是两个相对的概念，指调用的底层架构不同） 安装 JupyterNotebook。 尝试寒武纪开发的工具 MagicMind 和 YOLOv5。见 Cambricon(gitee.com)。 另一本很推荐的书是《智能计算系统》&#160;&#8617;&#xfe0e;"><meta property="og:type" content="article"><meta property="og:url" content="https://chenlinear.github.io/posts/20230304-gpu-dock/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-04T00:00:00+00:00"><meta itemprop=name content="寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上"><meta itemprop=description content="还未经测试!
显卡：寒武纪 MLU370-X4。
显卡坞的解决方式恐怕不行，因为如果重新更换设备需要重新从 ubuntu 开始装起，不如在另一台机器上构建一个本地服务器。除此之外的问题还有：笔记本自带的核显会打架吗？硬盘容量不够了。:(
视频介绍（除3.几乎完全是这个知乎专栏的内容）：
寒武纪 MLU370 加速卡简介及安装 寒武纪基础软件平台安装 MLU370 开发实战 文档（在熟悉视频介绍之后再查看）：
文档中心 MLU370-X4 智能加速卡产品手册 1.0.0 文档 我设想的本地服务器的构建流程：
安装 ubuntu，版本号见视频介绍：16.04或18.04? 按照视频介绍安装 torch.mlu 组件等等（和 Dive into Deep Learning1 这本书不同的地方在于，寒武纪在使用 pytorch 时导入 torch 和 torch.mlu 即可，不需安装 torchvision 即 pytorch 的 GPU 版本，此处 mlu 和 gpu 是两个相对的概念，指调用的底层架构不同） 安装 JupyterNotebook。 尝试寒武纪开发的工具 MagicMind 和 YOLOv5。见 Cambricon(gitee.com)。 另一本很推荐的书是《智能计算系统》&#160;&#8617;&#xfe0e;"><meta itemprop=datePublished content="2023-03-04T00:00:00+00:00"><meta itemprop=dateModified content="2023-03-04T00:00:00+00:00"><meta itemprop=wordCount content="54"><meta itemprop=keywords content="programming,"><link rel=canonical href=https://chenlinear.github.io/posts/20230304-gpu-dock/><link rel=icon href=https://chenlinear.github.io/assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Chen Li" href=https://chenlinear.github.io/atom.xml><link rel=alternate type=application/json title="Chen Li" href=https://chenlinear.github.io/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-align:justify;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:rbg(169,169,169,1);color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5{font-size:20px;font-weight:600;text-align:center}strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a:hover,a.heading-link{text-decoration:none}a,a:visited{color:#008b8b}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:circle}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:90ch;margin:0 auto}header{line-height:1;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:135px;width:135px;position:relative;margin:0 0 0 15px;float:right;border-radius:25%}table{border-collapse:collapse}table th,table td{border:1px solid #bebebe;padding:0 5px}.toc{margin:0 auto;width:100%;padding:0;margin-bottom:10px;background-color:#f9f9f9}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上","headline":"寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上","alternativeHeadline":"","description":"还未经测试!\n显卡：寒武纪 MLU370-X4。\n显卡坞的解决方式恐怕不行，因为如果重新更换设备需要重新从 ubuntu 开始装起，不如在另一台机器上构建一个本地服务器。除此之外的问题还有：笔记本自带的核显会打架吗？硬盘容量不够了。:(\n视频介绍（除3.几乎完全是这个知乎专栏的内容）：\n寒武纪 MLU370 加速卡简介及安装 寒武纪基础软件平台安装 MLU370 开发实战 文档（在熟悉视频介绍之后再查看）：\n文档中心 MLU370-X4 智能加速卡产品手册 1.0.0 文档 我设想的本地服务器的构建流程：\n安装 ubuntu，版本号见视频介绍：16.04或18.04? 按照视频介绍安装 torch.mlu 组件等等（和 Dive into Deep Learning1 这本书不同的地方在于，寒武纪在使用 pytorch 时导入 torch 和 torch.mlu 即可，不需安装 torchvision 即 pytorch 的 GPU 版本，此处 mlu 和 gpu 是两个相对的概念，指调用的底层架构不同） 安装 JupyterNotebook。 尝试寒武纪开发的工具 MagicMind 和 YOLOv5。见 Cambricon(gitee.com)。 另一本很推荐的书是《智能计算系统》\u0026#160;\u0026#x21a9;\u0026#xfe0e;","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/chenlinear.github.io\/posts\/20230304-gpu-dock\/"},"author":{"@type":"Person","name":"Chen Li"},"creator":{"@type":"Person","name":"Chen Li"},"accountablePerson":{"@type":"Person","name":"Chen Li"},"copyrightHolder":"Chen Li","copyrightYear":"2023","dateCreated":"2023-03-04T00:00:00.00Z","datePublished":"2023-03-04T00:00:00.00Z","dateModified":"2023-03-04T00:00:00.00Z","publisher":{"@type":"Organization","name":"Chen Li","url":"https://chenlinear.github.io","logo":{"@type":"ImageObject","url":"https:\/\/chenlinear.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://chenlinear.github.io/assets/favicon.ico","url":"https:\/\/chenlinear.github.io\/posts\/20230304-gpu-dock\/","wordCount":"54","genre":["programming"],"keywords":["programming"]}</script></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=/><b>Chen Li</b>
<span class="text-stone-500 animate-blink"></span></a></p><ul style=padding:0;margin:0><li><a href=/cv/><span>CV</span></a><li><a href=/posts/><span>Posts</span></a><li><a href=/about/><span>About</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>寒武纪 MLU370-X4 安装在本地服务器的 ubuntu 上</h2><div class=toc><nav id=TableOfContents></nav></div><p><strong>还未经测试!</strong></p><p>显卡：<a href="https://www.cambricon.com/index.php?m=content&amp;c=index&amp;a=lists&amp;catid=371">寒武纪 MLU370-X4</a>。</p><p>显卡坞的解决方式恐怕不行，因为如果重新更换设备需要重新从 ubuntu 开始装起，不如在另一台机器上构建一个本地服务器。除此之外的问题还有：笔记本自带的核显会打架吗？硬盘容量不够了。:(</p><p>视频介绍（除3.几乎完全是这个<a href=https://zhuanlan.zhihu.com/p/591827529>知乎专栏</a>的内容）：</p><ol><li><a href=https://www.bilibili.com/video/BV1Be4y1o7Jx/>寒武纪 MLU370 加速卡简介及安装</a></li><li><a href=https://www.bilibili.com/video/BV11G411G7ne/>寒武纪基础软件平台安装</a></li><li><a href="https://space.bilibili.com/503203932/channel/seriesdetail?sid=2724298">MLU370 开发实战</a></li></ol><p>文档（在熟悉视频介绍之后再查看）：</p><ol><li><a href=https://developer.cambricon.com/index/document/index/classid/3.html>文档中心</a></li><li><a href=https://www.cambricon.com/docs/product_docs/mlu370_x4/1.0.0/index.html>MLU370-X4 智能加速卡产品手册 1.0.0 文档</a></li></ol><p>我设想的本地服务器的构建流程：</p><ol><li>安装 ubuntu，版本号见视频介绍：16.04或18.04?</li><li>按照视频介绍安装 <code>torch.mlu</code> 组件等等（和 <a href=https://zh.d2l.ai/><em>Dive into Deep Learning</em></a><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> 这本书不同的地方在于，寒武纪在使用 pytorch 时导入 <code>torch</code> 和 <code>torch.mlu</code> 即可，不需安装 <code>torchvision</code> 即 pytorch 的 GPU 版本，此处 mlu 和 gpu 是两个相对的概念，指调用的底层架构不同）</li><li>安装 JupyterNotebook。</li><li>尝试寒武纪开发的工具 <a href=https://www.cambricon.com/docs/sdk_1.10.0/magicmind_1.1.0/user_guide/index.html>MagicMind</a> 和 YOLOv5。见 <a href=https://gitee.com/cambricon>Cambricon(gitee.com)</a>。</li></ol><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>另一本很推荐的书是《智能计算系统》&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><div class=post-date><span class="g time">March 4, 2023</span> &#8729;
<a href=https://chenlinear.github.io/tags/programming/>programming</a></div></section><div id=comments><script src=https://utteranc.es/client.js repo=chenlinear/chenlinear.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></main></body></html>