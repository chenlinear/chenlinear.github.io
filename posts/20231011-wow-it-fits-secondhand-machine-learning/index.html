<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Liste - https://ChenLi2049.github.io"><title>Wow It Fits! — Secondhand Machine Learning | Chen Li</title><meta name=description content="Chen Li's personal blog"><meta property="og:title" content="Wow It Fits! — Secondhand Machine Learning"><meta property="og:description" content="(There are a lot of pictures so it might take a while to load. This article is actually longer than it looks, because I use tabsets a lot.)
§1 Intro This section is about tensor (high-dimensional matrix) and torch.nn.
§1.1 Tensor In the rest of the article, we will always:
import torch import torch.nn as nn import torch.nn.functional as F from torchinfo import summary §1.1.1 Shape e.g. [H, W, C] (usually used in numpy or matplotlib."><meta property="og:type" content="article"><meta property="og:url" content="https://ChenLi2049.github.io/posts/20231011-wow-it-fits-secondhand-machine-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-11T00:00:00+00:00"><meta property="article:modified_time" content="2023-10-11T00:00:00+00:00"><meta itemprop=name content="Wow It Fits! — Secondhand Machine Learning"><meta itemprop=description content="(There are a lot of pictures so it might take a while to load. This article is actually longer than it looks, because I use tabsets a lot.)
§1 Intro This section is about tensor (high-dimensional matrix) and torch.nn.
§1.1 Tensor In the rest of the article, we will always:
import torch import torch.nn as nn import torch.nn.functional as F from torchinfo import summary §1.1.1 Shape e.g. [H, W, C] (usually used in numpy or matplotlib."><meta itemprop=datePublished content="2023-10-11T00:00:00+00:00"><meta itemprop=dateModified content="2023-10-11T00:00:00+00:00"><meta itemprop=wordCount content="9034"><meta itemprop=keywords content="programming,"><link rel=canonical href=https://ChenLi2049.github.io/posts/20231011-wow-it-fits-secondhand-machine-learning/><link rel=icon href=https://ChenLi2049.github.io/assets/favicon.ico><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Chen Li" href=https://ChenLi2049.github.io/atom.xml><link rel=alternate type=application/json title="Chen Li" href=https://ChenLi2049.github.io/feed.json><link rel="shortcut icon" type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-align:justify;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:rbg(169,169,169,1);color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5{font-size:20px;font-weight:600;text-align:center}strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a:hover,a.heading-link{text-decoration:none}a,a:visited{color:#008b8b}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:circle}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:90ch;margin:0 auto}header{line-height:1;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:135px;width:135px;position:relative;margin:0 0 0 15px;float:right;border-radius:25%}table{border-collapse:collapse}table th,table td{border:1px solid #bebebe;padding:0 5px}.toc{margin:0 auto;width:100%;padding:0;margin-bottom:10px;background-color:#f9f9f9}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Wow It Fits! — Secondhand Machine Learning","headline":"Wow It Fits! — Secondhand Machine Learning","alternativeHeadline":"","description":"(There are a lot of pictures so it might take a while to load. This article is actually longer than it looks, because I use tabsets a lot.)\n§1 Intro This section is about tensor (high-dimensional matrix) and torch.nn.\n§1.1 Tensor In the rest of the article, we will always:\nimport torch import torch.nn as nn import torch.nn.functional as F from torchinfo import summary §1.1.1 Shape e.g. [H, W, C] (usually used in numpy or matplotlib.","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/ChenLi2049.github.io\/posts\/20231011-wow-it-fits-secondhand-machine-learning\/"},"author":{"@type":"Person","name":"Chen Li"},"creator":{"@type":"Person","name":"Chen Li"},"accountablePerson":{"@type":"Person","name":"Chen Li"},"copyrightHolder":"Chen Li","copyrightYear":"2023","dateCreated":"2023-10-11T00:00:00.00Z","datePublished":"2023-10-11T00:00:00.00Z","dateModified":"2023-10-11T00:00:00.00Z","publisher":{"@type":"Organization","name":"Chen Li","url":"https://ChenLi2049.github.io","logo":{"@type":"ImageObject","url":"https:\/\/ChenLi2049.github.io\/assets\/favicon.ico","width":"32","height":"32"}},"image":"https://ChenLi2049.github.io/assets/favicon.ico","url":"https:\/\/ChenLi2049.github.io\/posts\/20231011-wow-it-fits-secondhand-machine-learning\/","wordCount":"9034","genre":["programming"],"keywords":["programming"]}</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/katex.min.css><script defer src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/katex.min.js></script>
<script defer src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script src=https://cdn.jsdelivr.net/npm/@xiee/utils/js/tabsets.min.js defer></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@xiee/utils/css/tabsets.min.css></head><body><a class=skip-link href=#main>Skip to main</a><main id=main><div class=content><header><p style=padding:0;margin:0><a href=/><b>Chen Li</b>
<span class="text-stone-500 animate-blink"></span></a></p><ul style=padding:0;margin:0><li><a href=/cv/><span>CV</span></a><li><a href=/posts/><span>Posts</span></a><li><a href=/about/><span>About</span></a></li></ul></header><hr class=hr-list style=padding:0;margin:0><section><h2 class=post>Wow It Fits! — Secondhand Machine Learning</h2><div class=toc><nav id=TableOfContents><ul><li><a href=#1-intro>§1 Intro</a><ul><li><a href=#11-tensor>§1.1 Tensor</a></li><li><a href=#12-torchnn>§1.2 <code>torch.nn</code></a></li></ul></li><li><a href=#2-cnn>§2 CNN</a><ul><li><a href=#21-mnist>§2.1 MNIST</a></li><li><a href=#22-alexnet-deep-learning-revolution>§2.2 AlexNet: Deep Learning Revolution</a></li><li><a href=#23-resnet-deeper>§2.3 ResNet: Deeper</a></li></ul></li><li><a href=#3-transformer>§3 Transformer</a><ul><li><a href=#31-embedding>§3.1 Embedding</a></li><li><a href=#32-transformer-encoder>§3.2 Transformer Encoder</a></li><li><a href=#33-encoder-decoder-encoder-only-decoder-only>§3.3 Encoder-Decoder, Encoder-Only, Decoder-Only</a></li><li><a href=#34-attention-is-all-you-need-the-original-transformer>§3.4 Attention is All You Need (the Original Transformer)</a></li><li><a href=#35-vision-transformer-vit>§3.5 Vision Transformer (ViT)</a></li><li><a href=#36-generative-pre-trained-transformer-gpt>§3.6 Generative Pre-trained Transformer (GPT)</a></li><li><a href=#37-variants>§3.7 Variants</a></li><li><a href=#38-mixture-of-experts-moe>§3.8 Mixture of Experts (MoE)</a></li><li><a href=#39-scaling-laws-emergence>§3.9 Scaling Laws, Emergence</a></li><li><a href=#310-transformers-are-cnns-rnns-gnns>§3.10 Transformers are CNNs, RNNs, GNNs</a></li></ul></li><li><a href=#4-fastai>§4 <code>fastai</code></a><ul><li><a href=#41-dataloaders>§4.1 <code>Dataloaders</code></a></li><li><a href=#42-learner>§4.2 <code>Learner</code></a></li></ul></li><li><a href=#5-transfer-learning>§5 Transfer Learning</a><ul><li><a href=#51-load-pretrained-resnet-vit>§5.1 Load Pretrained ResNet, ViT</a></li><li><a href=#52-acousticgravitational-wave-classification>§5.2 Acoustic/Gravitational Wave Classification</a></li><li><a href=#53-category-unknown-confidence-level>§5.3 Category &ldquo;Unknown&rdquo;, Confidence Level</a></li></ul></li></ul></nav></div><p>(There are a lot of pictures so it might take a while to load. This article is actually longer than it looks, because I use <a href=https://yihui.org/en/2023/10/section-tabsets/>tabsets</a> a lot.)</p><h2 id=1-intro>§1 Intro</h2><p>This section is about tensor (high-dimensional matrix) and <code>torch.nn</code>.</p><h3 id=11-tensor>§1.1 Tensor</h3><p>In the rest of the article, we will always:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn.functional</span> <span style=color:#00a8c8>as</span> <span style=color:#111>F</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchinfo</span> <span style=color:#f92672>import</span> <span style=color:#111>summary</span>
</span></span></code></pre></div><h4 id=111-shape>§1.1.1 Shape</h4><p>e.g. [H, W, C] (usually used in <code>numpy</code> or <code>matplotlib.pyplot</code>) or [C, H, W] (usually used in <code>torch</code>) or [batch_size, C, H, W].</p><p><img src=https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png alt loading=lazy decoding=async class=full-width></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, C, H, W]</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 3, 32, 32<span style=color:#f92672>])</span>
</span></span></code></pre></div><p>Commonly used method to change the shape of a tensor:</p><ul><li><a href=https://pytorch.org/docs/stable/generated/torch.Tensor.view.html><code>.view()</code></a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.reshape.html><code>.reshape()</code></a> (<code>.reshape()</code> is for non-contiguous tensor, try <code>.view()</code> first. <code>x.reshape()</code> is equal to <code>x.contiguous().view()</code>)</li><li><a href=https://pytorch.org/docs/stable/generated/torch.unsqueeze.html><code>.unsqueeze()</code></a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.squeeze.html><code>.squeeze()</code></a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.transpose.html><code>.transpose()</code></a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.permute.html><code>.permute()</code></a></li></ul><p><a href=https://github.com/arogozhnikov/einops><code>einops</code></a> provides a more intuitive way to change the shape.</p><h4 id=112-device>§1.1.2 Device</h4><p><a href=https://pytorch.org/docs/stable/tensor_attributes.html#torch-device><code>torch.device</code></a></p><div class=tabset></div><ul><li><p>Tensor device</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cpu&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cpu
</span></span><span style=display:flex><span>cuda:0
</span></span><span style=display:flex><span>cpu
</span></span></code></pre></div></li><li><p>Model device</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Model</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>Model</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span></code></pre></div></li><li><p>Be on the same device</p><p>All tensors and objects (datasets, models) should be on the same device.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Model</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>32</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>32</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>128</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>10</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>),</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>Model</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 10<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><h4 id=113-type>§1.1.3 Type</h4><p><a href=https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype><code>torch.dtype</code></a></p><div class=tabset></div><ul><li><p><code>numpy.ndarray</code> -> <code>torch.Tensor</code>:</p><p>float64 -> float32</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>random</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>from_numpy</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>float32</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>float64
</span></span><span style=display:flex><span>torch.float64
</span></span><span style=display:flex><span>cpu
</span></span><span style=display:flex><span>torch.float32
</span></span></code></pre></div></li><li><p><code>torch.Tensor</code> -> <code>numpy.ndarray</code>:</p><p><code>cuda</code> -> <code>cpu</code>, float32 -> float64</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>,</span> <span style=color:#ae81ff>32</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cpu&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>numpy</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>astype</span><span style=color:#111>(</span><span style=color:#d88200>&#39;float64&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>dtype</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.float32
</span></span><span style=display:flex><span>cuda:0
</span></span><span style=display:flex><span>float32
</span></span><span style=display:flex><span>float64
</span></span></code></pre></div></li></ul><h3 id=12-torchnn>§1.2 <code>torch.nn</code></h3><div class=tabset></div><ul><li><p><code>nn.Conv2d</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html><code>nn.Conv2d</code></a>. Convolution is a kind of weighted mean.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>conv2d</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>conv2d</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 3, 28, 28<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 12, 26, 26<span style=color:#f92672>])</span>
</span></span></code></pre></div></li><li><p><code>nn.MaxPool2d</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html><code>nn.MaxPool2d</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>pool</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>MaxPool2d</span><span style=color:#111>(</span><span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>pool</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 3, 28, 28<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 3, 14, 14<span style=color:#f92672>])</span>
</span></span></code></pre></div></li><li><p><code>nn.BatchNorm2d</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html><code>nn.BatchNorm2d</code></a>, See Fig.2 of <a href=https://arxiv.org/abs/1803.08494>[1803.08494] <em>Group Normalization</em></a>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>batchnorm</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>BatchNorm2d</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>batchnorm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span></code></pre></div></li><li><p><code>nn.Linear</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Linear.html><code>nn.Linear</code></a>. For fully connected layer.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>linear</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>12</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>linear</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>128, 12<span style=color:#f92672>])</span>
</span></span></code></pre></div></li><li><p><code>nn.Dropout</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html><code>nn.Dropout</code></a>. For fully connected layer. Using the samples in the Bernoulli distribution, some elements of the input tensor are randomly zeroed with probability $p$. To use it:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>p</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span><span style=color:#111>,</span> <span style=color:#111>inplace</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><code>x</code> can be a tensor in any shape.</p></li><li><p><code>nn.ReLU</code> or <code>F.relu</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html><code>nn.ReLU</code></a>, <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html><code>F.relu</code></a>. Activation function, $\text{ReLU}(x)=\max{(0,x)}$, to use it：</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ReLU</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>or:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><code>x</code> can be a tensor in any shape.</p></li><li><p><code>nn.RNN</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html><code>nn.RNN</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>input_size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span><span style=color:#111>hidden_size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span><span style=color:#111>num_layers</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span><span style=color:#111>seq_length</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span><span style=color:#111>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#111>rnn</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>RNN</span><span style=color:#111>(</span><span style=color:#111>input_size</span><span style=color:#111>,</span> <span style=color:#111>hidden_size</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input_data</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>input_size</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>hidden_state</span> <span style=color:#f92672>=</span> <span style=color:#111>rnn</span><span style=color:#111>(</span><span style=color:#111>input_data</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>5, 3, 20<span style=color:#f92672>])</span>
</span></span></code></pre></div></li><li><p><code>nn.Module</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html><code>nn.Module</code></a>. Construct a block of layers. It could be the entire model or just a block of the entire model or loss function, etc.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyBlock</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># define every layer</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># define forward propagation</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div></li><li><p><code>nn.Sequential</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html><code>nn.Sequential</code></a>. Compared with <code>nn.Module</code>, <code>nn.Sequential</code> can add the layers more easily and don&rsquo;t have to define forward propagation. This is more useful when building a simple neural network</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Sequential</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>    <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>    <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ReLU</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>    <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>64</span><span style=color:#111>,</span> <span style=color:#ae81ff>5</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>y</span> <span style=color:#f92672>=</span> <span style=color:#111>model</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>y</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 64, 22, 22<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><h2 id=2-cnn>§2 CNN</h2><p>MNIST is here for the purpose of introducing the pipeline of Machine Learning; AlexNet showed the power of cuda and deep neural network; ResNet is the most popular CNN to this day and residual connections are also used in Transformers.</p><p>| <a href=https://poloclub.github.io/cnn-explainer/>CNN Explainer</a> | <a href=https://www.shadertoy.com/view/msVXWD>Handwritten Digit Recognizer CNN</a> |</p><h3 id=21-mnist>§2.1 MNIST</h3><p>| <a href=https://github.com/pytorch/examples/tree/main/mnist>mnist (torch)</a> | <a href=https://pytorch.org/tutorials/beginner/nn_tutorial.html><em>What is torch.nn really?</em></a> | <a href=https://paperswithcode.com/sota/image-classification-on-mnist>MNIST Benchmark</a> | <a href=https://karpathy.github.io/2022/03/14/lecun1989/><em>Deep Neural Nets: 33 years ago and 33 years from now</em></a> |</p><center><img src=https://production-media.paperswithcode.com/datasets/MNIST-0000000001-2e09631a_09liOmx.jpg></center><p>In <a href=https://github.com/pytorch/examples/tree/main/mnist>mnist (torch)</a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Net</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>train</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>test</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>main</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>if</span> <span style=color:#111>__name__</span> <span style=color:#f92672>==</span> <span style=color:#d88200>&#39;__main__&#39;</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>    <span style=color:#111>main</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=tabset></div><ul><li><p>Cross Entropy Loss</p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html><code>F.log_softmax</code></a>, <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html><code>F.nll_loss</code></a>, <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html><code>F.cross_entropy</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>pred</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>10</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, num_classes]</span>
</span></span><span style=display:flex><span><span style=color:#111>target</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randint</span><span style=color:#111>(</span><span style=color:#ae81ff>10</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,))</span><span style=color:#75715e># [batch_size,]</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>nll_loss</span><span style=color:#111>(</span><span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>log_softmax</span><span style=color:#111>(</span><span style=color:#111>pred</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>cross_entropy</span><span style=color:#111>(</span><span style=color:#111>pred</span><span style=color:#111>,</span> <span style=color:#111>target</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensor<span style=color:#f92672>(</span>2.6026<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>(</span>2.6026<span style=color:#f92672>)</span><span style=color:#75715e># same result</span>
</span></span></code></pre></div></li><li><p><code>class Net</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Net</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>        <span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>log_softmax</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>output</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>Net</span><span style=color:#111>(),</span> <span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>,</span> <span style=color:#ae81ff>28</span><span style=color:#111>))</span><span style=color:#75715e># [batch_size, C, H, W]</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                   Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Net                                      <span style=color:#f92672>[</span>16, 10<span style=color:#f92672>]</span>                  --
</span></span><span style=display:flex><span>├─Conv2d: 1-1                            <span style=color:#f92672>[</span>16, 32, 26, 26<span style=color:#f92672>]</span>          <span style=color:#ae81ff>320</span>
</span></span><span style=display:flex><span>├─Conv2d: 1-2                            <span style=color:#f92672>[</span>16, 64, 24, 24<span style=color:#f92672>]</span>          18,496
</span></span><span style=display:flex><span>├─Dropout: 1-3                           <span style=color:#f92672>[</span>16, 64, 12, 12<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>├─Linear: 1-4                            <span style=color:#f92672>[</span>16, 128<span style=color:#f92672>]</span>                 1,179,776
</span></span><span style=display:flex><span>├─Dropout: 1-5                           <span style=color:#f92672>[</span>16, 128<span style=color:#f92672>]</span>                 --
</span></span><span style=display:flex><span>├─Linear: 1-6                            <span style=color:#f92672>[</span>16, 10<span style=color:#f92672>]</span>                  1,290
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Total params: 1,199,882
</span></span><span style=display:flex><span>Trainable params: 1,199,882
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>M<span style=color:#f92672>)</span>: 192.82
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 0.05
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 7.51
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 4.80
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 12.35
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span></code></pre></div></li><li><p><code>def train</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>train</span><span style=color:#111>(</span><span style=color:#111>args</span><span style=color:#111>,</span> <span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#111>,</span> <span style=color:#111>train_loader</span><span style=color:#111>,</span> <span style=color:#111>optimizer</span><span style=color:#111>,</span> <span style=color:#111>epoch</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># set the model to training mode: activate dropout and batch normalization.</span>
</span></span><span style=display:flex><span>    <span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>train</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># go through each batch.</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>batch_idx</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#111>data</span><span style=color:#111>,</span> <span style=color:#111>target</span><span style=color:#111>)</span> <span style=color:#f92672>in</span> <span style=color:#111>enumerate</span><span style=color:#111>(</span><span style=color:#111>train_loader</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># put data and target to device.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>data</span><span style=color:#111>,</span> <span style=color:#111>target</span> <span style=color:#f92672>=</span> <span style=color:#111>data</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>device</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the optimizer&#39;s gradient is reset to 0.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>optimizer</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># forward pass.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>model</span><span style=color:#111>(</span><span style=color:#111>data</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># calculate loss.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>nll_loss</span><span style=color:#111>(</span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>target</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># calculate the gradients.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># backward propagation.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>optimizer</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># print loss</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>batch_idx</span> <span style=color:#f92672>%</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>log_interval</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>&#39;Train Epoch: </span><span style=color:#d88200>{}</span><span style=color:#d88200> [</span><span style=color:#d88200>{}</span><span style=color:#d88200>/</span><span style=color:#d88200>{}</span><span style=color:#d88200> (</span><span style=color:#d88200>{:.0f}</span><span style=color:#d88200>%)]</span><span style=color:#8045ff>\t</span><span style=color:#d88200>Loss: </span><span style=color:#d88200>{:.6f}</span><span style=color:#d88200>&#39;</span><span style=color:#f92672>.</span><span style=color:#111>format</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>                <span style=color:#111>epoch</span><span style=color:#111>,</span> <span style=color:#111>batch_idx</span> <span style=color:#f92672>*</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>data</span><span style=color:#111>),</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>train_loader</span><span style=color:#f92672>.</span><span style=color:#111>dataset</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>                <span style=color:#ae81ff>100.</span> <span style=color:#f92672>*</span> <span style=color:#111>batch_idx</span> <span style=color:#f92672>/</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>train_loader</span><span style=color:#111>),</span> <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>item</span><span style=color:#111>()))</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># if `dry_run`, only run 1 epoch.</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>if</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>dry_run</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>                <span style=color:#00a8c8>break</span>
</span></span></code></pre></div></li><li><p><code>def test</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>test</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#111>,</span> <span style=color:#111>test_loader</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># set the model to evaluation mode.</span>
</span></span><span style=display:flex><span>    <span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>eval</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#111>test_loss</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#111>correct</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># gradient calculations are disabled.</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>with</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>no_grad</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>data</span><span style=color:#111>,</span> <span style=color:#111>target</span> <span style=color:#f92672>in</span> <span style=color:#111>test_loader</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># put data and target to device.</span>
</span></span><span style=display:flex><span>            <span style=color:#111>data</span><span style=color:#111>,</span> <span style=color:#111>target</span> <span style=color:#f92672>=</span> <span style=color:#111>data</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>device</span><span style=color:#111>),</span> <span style=color:#111>target</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># forward pass.</span>
</span></span><span style=display:flex><span>            <span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>model</span><span style=color:#111>(</span><span style=color:#111>data</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># calculate loss, sum up batch loss.</span>
</span></span><span style=display:flex><span>            <span style=color:#111>test_loss</span> <span style=color:#f92672>+=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>nll_loss</span><span style=color:#111>(</span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>target</span><span style=color:#111>,</span> <span style=color:#111>reduction</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;sum&#39;</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>item</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># get the index of the max log-probability.</span>
</span></span><span style=display:flex><span>            <span style=color:#111>pred</span> <span style=color:#f92672>=</span> <span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>argmax</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>keepdim</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># compare predicted labels with target labels.</span>
</span></span><span style=display:flex><span>            <span style=color:#111>correct</span> <span style=color:#f92672>+=</span> <span style=color:#111>pred</span><span style=color:#f92672>.</span><span style=color:#111>eq</span><span style=color:#111>(</span><span style=color:#111>target</span><span style=color:#f92672>.</span><span style=color:#111>view_as</span><span style=color:#111>(</span><span style=color:#111>pred</span><span style=color:#111>))</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>item</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># average loss per sample.</span>
</span></span><span style=display:flex><span>    <span style=color:#111>test_loss</span> <span style=color:#f92672>/=</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>test_loader</span><span style=color:#f92672>.</span><span style=color:#111>dataset</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># print</span>
</span></span><span style=display:flex><span>    <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>&#39;</span><span style=color:#8045ff>\n</span><span style=color:#d88200>Test set: Average loss: </span><span style=color:#d88200>{:.4f}</span><span style=color:#d88200>, Accuracy: </span><span style=color:#d88200>{}</span><span style=color:#d88200>/</span><span style=color:#d88200>{}</span><span style=color:#d88200> (</span><span style=color:#d88200>{:.0f}</span><span style=color:#d88200>%)</span><span style=color:#8045ff>\n</span><span style=color:#d88200>&#39;</span><span style=color:#f92672>.</span><span style=color:#111>format</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>        <span style=color:#111>test_loss</span><span style=color:#111>,</span> <span style=color:#111>correct</span><span style=color:#111>,</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>test_loader</span><span style=color:#f92672>.</span><span style=color:#111>dataset</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>100.</span> <span style=color:#f92672>*</span> <span style=color:#111>correct</span> <span style=color:#f92672>/</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>test_loader</span><span style=color:#f92672>.</span><span style=color:#111>dataset</span><span style=color:#111>)))</span>
</span></span></code></pre></div></li><li><p><code>def main</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>main</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Training settings</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span> <span style=color:#f92672>=</span> <span style=color:#111>argparse</span><span style=color:#f92672>.</span><span style=color:#111>ArgumentParser</span><span style=color:#111>(</span><span style=color:#111>description</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;PyTorch MNIST Example&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--batch-size&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>int</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>64</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;N&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;input batch size for training (default: 64)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--test-batch-size&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>int</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;N&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;input batch size for testing (default: 1000)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--epochs&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>int</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>14</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;N&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;number of epochs to train (default: 14)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--lr&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>float</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;LR&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;learning rate (default: 1.0)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--gamma&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>float</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;M&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;Learning rate step gamma (default: 0.7)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--no-cuda&#39;</span><span style=color:#111>,</span> <span style=color:#111>action</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;store_true&#39;</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;disables CUDA training&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--no-mps&#39;</span><span style=color:#111>,</span> <span style=color:#111>action</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;store_true&#39;</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;disables macOS GPU training&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--dry-run&#39;</span><span style=color:#111>,</span> <span style=color:#111>action</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;store_true&#39;</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;quickly check a single pass&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--seed&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>int</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;S&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;random seed (default: 1)&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--log-interval&#39;</span><span style=color:#111>,</span> <span style=color:#111>type</span><span style=color:#f92672>=</span><span style=color:#111>int</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#ae81ff>10</span><span style=color:#111>,</span> <span style=color:#111>metavar</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;N&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;how many batches to wait before logging training status&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>add_argument</span><span style=color:#111>(</span><span style=color:#d88200>&#39;--save-model&#39;</span><span style=color:#111>,</span> <span style=color:#111>action</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;store_true&#39;</span><span style=color:#111>,</span> <span style=color:#111>default</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#111>help</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;For Saving the current Model&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>args</span> <span style=color:#f92672>=</span> <span style=color:#111>parser</span><span style=color:#f92672>.</span><span style=color:#111>parse_args</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#111>use_cuda</span> <span style=color:#f92672>=</span> <span style=color:#f92672>not</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>no_cuda</span> <span style=color:#f92672>and</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cuda</span><span style=color:#f92672>.</span><span style=color:#111>is_available</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#111>use_mps</span> <span style=color:#f92672>=</span> <span style=color:#f92672>not</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>no_mps</span> <span style=color:#f92672>and</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>backends</span><span style=color:#f92672>.</span><span style=color:#111>mps</span><span style=color:#f92672>.</span><span style=color:#111>is_available</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>manual_seed</span><span style=color:#111>(</span><span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>seed</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>if</span> <span style=color:#111>use_cuda</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>device</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>(</span><span style=color:#d88200>&#34;cuda&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>elif</span> <span style=color:#111>use_mps</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>device</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>(</span><span style=color:#d88200>&#34;mps&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>else</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>device</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>(</span><span style=color:#d88200>&#34;cpu&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#111>train_kwargs</span> <span style=color:#f92672>=</span> <span style=color:#111>{</span><span style=color:#d88200>&#39;batch_size&#39;</span><span style=color:#111>:</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>batch_size</span><span style=color:#111>}</span>
</span></span><span style=display:flex><span>    <span style=color:#111>test_kwargs</span> <span style=color:#f92672>=</span> <span style=color:#111>{</span><span style=color:#d88200>&#39;batch_size&#39;</span><span style=color:#111>:</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>test_batch_size</span><span style=color:#111>}</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>if</span> <span style=color:#111>use_cuda</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>cuda_kwargs</span> <span style=color:#f92672>=</span> <span style=color:#111>{</span><span style=color:#d88200>&#39;num_workers&#39;</span><span style=color:#111>:</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                       <span style=color:#d88200>&#39;pin_memory&#39;</span><span style=color:#111>:</span> <span style=color:#00a8c8>True</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                       <span style=color:#d88200>&#39;shuffle&#39;</span><span style=color:#111>:</span> <span style=color:#00a8c8>True</span><span style=color:#111>}</span>
</span></span><span style=display:flex><span>        <span style=color:#111>train_kwargs</span><span style=color:#f92672>.</span><span style=color:#111>update</span><span style=color:#111>(</span><span style=color:#111>cuda_kwargs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>test_kwargs</span><span style=color:#f92672>.</span><span style=color:#111>update</span><span style=color:#111>(</span><span style=color:#111>cuda_kwargs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># https://pytorch.org/vision/stable/transforms.html</span>
</span></span><span style=display:flex><span>    <span style=color:#111>transform</span><span style=color:#f92672>=</span><span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>Compose</span><span style=color:#111>([</span>
</span></span><span style=display:flex><span>        <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>ToTensor</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>        <span style=color:#111>transforms</span><span style=color:#f92672>.</span><span style=color:#111>Normalize</span><span style=color:#111>((</span><span style=color:#ae81ff>0.1307</span><span style=color:#111>,),</span> <span style=color:#111>(</span><span style=color:#ae81ff>0.3081</span><span style=color:#111>,))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>])</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># load training dataset and testing dataset</span>
</span></span><span style=display:flex><span>    <span style=color:#111>dataset1</span> <span style=color:#f92672>=</span> <span style=color:#111>datasets</span><span style=color:#f92672>.</span><span style=color:#111>MNIST</span><span style=color:#111>(</span><span style=color:#d88200>&#39;../data&#39;</span><span style=color:#111>,</span> <span style=color:#111>train</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>,</span> <span style=color:#111>download</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                       <span style=color:#111>transform</span><span style=color:#f92672>=</span><span style=color:#111>transform</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>dataset2</span> <span style=color:#f92672>=</span> <span style=color:#111>datasets</span><span style=color:#f92672>.</span><span style=color:#111>MNIST</span><span style=color:#111>(</span><span style=color:#d88200>&#39;../data&#39;</span><span style=color:#111>,</span> <span style=color:#111>train</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                       <span style=color:#111>transform</span><span style=color:#f92672>=</span><span style=color:#111>transform</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>train_loader</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>utils</span><span style=color:#f92672>.</span><span style=color:#111>data</span><span style=color:#f92672>.</span><span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset1</span><span style=color:#111>,</span><span style=color:#f92672>**</span><span style=color:#111>train_kwargs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>test_loader</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>utils</span><span style=color:#f92672>.</span><span style=color:#111>data</span><span style=color:#f92672>.</span><span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset2</span><span style=color:#111>,</span> <span style=color:#f92672>**</span><span style=color:#111>test_kwargs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># put model to device</span>
</span></span><span style=display:flex><span>    <span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>Net</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># set optimizer and scheduler</span>
</span></span><span style=display:flex><span>    <span style=color:#111>optimizer</span> <span style=color:#f92672>=</span> <span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>Adadelta</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>lr</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>scheduler</span> <span style=color:#f92672>=</span> <span style=color:#111>StepLR</span><span style=color:#111>(</span><span style=color:#111>optimizer</span><span style=color:#111>,</span> <span style=color:#111>step_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>gamma</span><span style=color:#f92672>=</span><span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>gamma</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># train and test</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>epoch</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>epochs</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>train</span><span style=color:#111>(</span><span style=color:#111>args</span><span style=color:#111>,</span> <span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#111>,</span> <span style=color:#111>train_loader</span><span style=color:#111>,</span> <span style=color:#111>optimizer</span><span style=color:#111>,</span> <span style=color:#111>epoch</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>test</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#111>,</span> <span style=color:#111>test_loader</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>scheduler</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># save model</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>if</span> <span style=color:#111>args</span><span style=color:#f92672>.</span><span style=color:#111>save_model</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>        <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>save</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>state_dict</span><span style=color:#111>(),</span> <span style=color:#d88200>&#34;mnist_cnn.pt&#34;</span><span style=color:#111>)</span>
</span></span></code></pre></div></li></ul><p>Later we will use <code>fastai</code> instead of writing <code>def train</code>, <code>def test</code>, <code>def main</code> from scratch.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python main.py
</span></span></code></pre></div><p>will get (full log see <a href=20231011-wow-it-fits-secondhand-machine-learning-02_mnist.log>02_mnist.log</a>):</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz
</span></span><span style=display:flex><span>100% 9912422/9912422 <span style=color:#f92672>[</span>00:00&lt;00:00, 96238958.45it/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz
</span></span><span style=display:flex><span>100% 28881/28881 <span style=color:#f92672>[</span>00:00&lt;00:00, 151799115.07it/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz
</span></span><span style=display:flex><span>100% 1648877/1648877 <span style=color:#f92672>[</span>00:00&lt;00:00, 27617389.31it/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
</span></span><span style=display:flex><span>Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz
</span></span><span style=display:flex><span>100% 4542/4542 <span style=color:#f92672>[</span>00:00&lt;00:00, 20180644.88it/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>1</span> <span style=color:#f92672>[</span>0/60000 <span style=color:#f92672>(</span>0%<span style=color:#f92672>)]</span>	Loss: 2.282550
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>1</span> <span style=color:#f92672>[</span>640/60000 <span style=color:#f92672>(</span>1%<span style=color:#f92672>)]</span>	Loss: 1.384441
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>1</span> <span style=color:#f92672>[</span>58880/60000 <span style=color:#f92672>(</span>98%<span style=color:#f92672>)]</span>	Loss: 0.064402
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>1</span> <span style=color:#f92672>[</span>59520/60000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)]</span>	Loss: 0.033435
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Test set: Average loss: 0.0468, Accuracy: 9842/10000 <span style=color:#f92672>(</span>98%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>2</span> <span style=color:#f92672>[</span>0/60000 <span style=color:#f92672>(</span>0%<span style=color:#f92672>)]</span>	Loss: 0.098867
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>2</span> <span style=color:#f92672>[</span>640/60000 <span style=color:#f92672>(</span>1%<span style=color:#f92672>)]</span>	Loss: 0.016046
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>2</span> <span style=color:#f92672>[</span>58880/60000 <span style=color:#f92672>(</span>98%<span style=color:#f92672>)]</span>	Loss: 0.108346
</span></span><span style=display:flex><span>Train Epoch: <span style=color:#ae81ff>2</span> <span style=color:#f92672>[</span>59520/60000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)]</span>	Loss: 0.108657
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Test set: Average loss: 0.0327, Accuracy: 9894/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0346, Accuracy: 9887/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0314, Accuracy: 9891/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0301, Accuracy: 9903/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0301, Accuracy: 9913/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0293, Accuracy: 9918/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0295, Accuracy: 9919/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0296, Accuracy: 9915/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0277, Accuracy: 9919/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0284, Accuracy: 9922/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0272, Accuracy: 9922/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0278, Accuracy: 9921/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Test set: Average loss: 0.0278, Accuracy: 9922/10000 <span style=color:#f92672>(</span>99%<span style=color:#f92672>)</span>
</span></span></code></pre></div><h3 id=22-alexnet-deep-learning-revolution>§2.2 AlexNet: Deep Learning Revolution</h3><p><a href=https://www.image-net.org/>ImageNet</a>: 14,197,122 images, 21841 synsets indexed.</p><p>| <a href=https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html>paper</a> | <a href=https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py><code>torchvision.models.alexnet</code></a> | <a href=https://pytorch.org/hub/pytorch_vision_alexnet/>AlexNet (pytorch.org)</a> |</p><table><thead><tr><th style=text-align:left>Methods</th><th style=text-align:left>Do we use it today?</th></tr></thead><tbody><tr><td style=text-align:left>2 GPUs: written in <code>cuda</code>, split into 2 different pipelines with connection.</td><td style=text-align:left>✔️&✖️</td></tr><tr><td style=text-align:left>Simple activation function <a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>ReLU</a> ($\text{ReLU} (x) = \max{(0,x)}$), instead of <a href=https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html>Tanh</a> ($\text{Tanh} (x) = \tanh{(x)}$) or <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html>Sigmoid</a> ($\sigma (x)= (1+e^{-x})^{-1}$).</td><td style=text-align:left>✔️</td></tr><tr><td style=text-align:left>Local response normalization</td><td style=text-align:left>✖️</td></tr><tr><td style=text-align:left>Overlapping pooling</td><td style=text-align:left>✖️</td></tr><tr><td style=text-align:left>The feature map ($C$) keeps increasing (3 $\to$ 48 $\to$ 128 $\to$ 192 $\to$ 128), while the resolution ($H$, $W$) keeps decreasing (224 $\to$ 55 $\to$ 27 $\to$ 13 $\to$ 13 $\to$ 13).</td><td style=text-align:left>✔️</td></tr><tr><td style=text-align:left>Kernel size keeps decreasing (11 $\to$ 5 $\to$ 3 $\to$ 3 $\to$ 3)</td><td style=text-align:left>✖️, same kernel size 3, see ResNet below</td></tr><tr><td style=text-align:left>Multiple linear layers. (take most of the parameters, 55M/61M)</td><td style=text-align:left>✖️</td></tr><tr><td style=text-align:left>Data augmentation (Image translations and horizontal reflections, color jitter)</td><td style=text-align:left><a href=https://pytorch.org/vision/stable/transforms.html>✔️</a>, actually this is more data</td></tr><tr><td style=text-align:left>Dropout</td><td style=text-align:left>✔️</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>AlexNet</span><span style=color:#111>(),</span> <span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                   Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>AlexNet                                  <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>├─Sequential: 1-1                        <span style=color:#f92672>[</span>16, 256, 6, 6<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-1                       <span style=color:#f92672>[</span>16, 64, 55, 55<span style=color:#f92672>]</span>          23,296
</span></span><span style=display:flex><span>│    └─ReLU: 2-2                         <span style=color:#f92672>[</span>16, 64, 55, 55<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─MaxPool2d: 2-3                    <span style=color:#f92672>[</span>16, 64, 27, 27<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-4                       <span style=color:#f92672>[</span>16, 192, 27, 27<span style=color:#f92672>]</span>         307,392
</span></span><span style=display:flex><span>│    └─ReLU: 2-5                         <span style=color:#f92672>[</span>16, 192, 27, 27<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─MaxPool2d: 2-6                    <span style=color:#f92672>[</span>16, 192, 13, 13<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-7                       <span style=color:#f92672>[</span>16, 384, 13, 13<span style=color:#f92672>]</span>         663,936
</span></span><span style=display:flex><span>│    └─ReLU: 2-8                         <span style=color:#f92672>[</span>16, 384, 13, 13<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-9                       <span style=color:#f92672>[</span>16, 256, 13, 13<span style=color:#f92672>]</span>         884,992
</span></span><span style=display:flex><span>│    └─ReLU: 2-10                        <span style=color:#f92672>[</span>16, 256, 13, 13<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-11                      <span style=color:#f92672>[</span>16, 256, 13, 13<span style=color:#f92672>]</span>         590,080
</span></span><span style=display:flex><span>│    └─ReLU: 2-12                        <span style=color:#f92672>[</span>16, 256, 13, 13<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─MaxPool2d: 2-13                   <span style=color:#f92672>[</span>16, 256, 6, 6<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─AdaptiveAvgPool2d: 1-2                 <span style=color:#f92672>[</span>16, 256, 6, 6<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─Sequential: 1-3                        <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>│    └─Dropout: 2-14                     <span style=color:#f92672>[</span>16, 9216<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>│    └─Linear: 2-15                      <span style=color:#f92672>[</span>16, 4096<span style=color:#f92672>]</span>                37,752,832
</span></span><span style=display:flex><span>│    └─ReLU: 2-16                        <span style=color:#f92672>[</span>16, 4096<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>│    └─Dropout: 2-17                     <span style=color:#f92672>[</span>16, 4096<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>│    └─Linear: 2-18                      <span style=color:#f92672>[</span>16, 4096<span style=color:#f92672>]</span>                16,781,312
</span></span><span style=display:flex><span>│    └─ReLU: 2-19                        <span style=color:#f92672>[</span>16, 4096<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>│    └─Linear: 2-20                      <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                4,097,000
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Total params: 61,100,840
</span></span><span style=display:flex><span>Trainable params: 61,100,840
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>G<span style=color:#f92672>)</span>: 11.43
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 9.63
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 63.26
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 244.40
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 317.29
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span></code></pre></div><h3 id=23-resnet-deeper>§2.3 ResNet: Deeper</h3><p>| <a href=https://arxiv.org/abs/1512.03385>paper</a> | <a href=https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py><code>torchvision.models.resnet</code></a> | <a href=https://pytorch.org/hub/pytorch_vision_resnet/>ResNet (pytorch.org)</a> |</p><p>Problem: With deeper layers, the loss goes upwards (see Fig.1 of the paper), but even if all the added layers are identity functions, the loss would be the same.</p><table><thead><tr><th style=text-align:left>Methods</th><th style=text-align:left>Do we use it today?</th></tr></thead><tbody><tr><td style=text-align:left>Residual connections to learn the differences and go <strong>deeper</strong> (50, 101, 152, 1202 layers, with 0.85M parameters to 19.4M parameters)</td><td style=text-align:left>✔️</td></tr><tr><td style=text-align:left>The feature map ($C$) keeps increasing (64 $\to$ 128 $\to$ 256 $\to$ 512), while the number of the resolution ($H$, $W$) keeps decreasing (224 $\to$ 112 $\to$ 56 $\to$ 28 $\to$ 14 $\to$ 7 $\to$ 1).</td><td style=text-align:left>✔️</td></tr><tr><td style=text-align:left>Stride 2 convolution kernel, instead of pooling</td><td style=text-align:left>✔️</td></tr><tr><td style=text-align:left>Bottleneck building block: $1 \times 1$ convolution kernel</td><td style=text-align:left>✔️&✖️</td></tr><tr><td style=text-align:left>Adopt batch normalization (BN) right after each convolution and before activation</td><td style=text-align:left>✔️&✖️, ongoing debate</td></tr></tbody></table><p>Basically residual is:</p><p><img src=20231011-wow-it-fits-secondhand-machine-learning-residual-waste.jpg alt loading=lazy decoding=async class=full-width></p><p>Sorry, not that &ldquo;residual&rdquo;. 🤣</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Res</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#f92672>...</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#f92672>...</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>By using residual connections, the model will learn linearity first and non-linearity after. We will see residual connections in Transformers.</p><div class=tabset></div><ul><li><p><code>torchvision.models.resnet</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>conv3x3</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>conv1x1</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>BasicBlock</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Bottleneck</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>ResNet</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>_make_layer</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>_forward_impl</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>ResNet18_Weights</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>resnet18</span><span style=color:#111>():</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><p>To use it:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision.models.resnet</span> <span style=color:#f92672>import</span> <span style=color:#111>resnet18</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>resnet18</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>hub</span><span style=color:#f92672>.</span><span style=color:#111>load</span><span style=color:#111>(</span><span style=color:#d88200>&#39;pytorch/vision:v0.10.0&#39;</span><span style=color:#111>,</span> <span style=color:#d88200>&#39;resnet18&#39;</span><span style=color:#111>,</span> <span style=color:#111>pretrained</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Downloading: <span style=color:#d88200>&#34;https://github.com/pytorch/vision/zipball/v0.10.0&#34;</span> to /root/.cache/torch/hub/v0.10.0.zip
</span></span><span style=display:flex><span>/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter <span style=color:#d88200>&#39;pretrained&#39;</span> is deprecated since 0.13 and may be removed in the future, please use <span style=color:#d88200>&#39;weights&#39;</span> instead.
</span></span><span style=display:flex><span>  warnings.warn<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or <span style=color:#d88200>`</span>None<span style=color:#d88200>`</span> <span style=color:#00a8c8>for</span> <span style=color:#d88200>&#39;weights&#39;</span> are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing <span style=color:#d88200>`</span><span style=color:#111>weights</span><span style=color:#f92672>=</span>ResNet18_Weights.IMAGENET1K_V1<span style=color:#d88200>`</span>. You can also use <span style=color:#d88200>`</span><span style=color:#111>weights</span><span style=color:#f92672>=</span>ResNet18_Weights.DEFAULT<span style=color:#d88200>`</span> to get the most up-to-date weights.
</span></span><span style=display:flex><span>  warnings.warn<span style=color:#f92672>(</span>msg<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>Downloading: <span style=color:#d88200>&#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34;</span> to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
</span></span><span style=display:flex><span>100%<span style=color:#111>|</span>██████████<span style=color:#111>|</span> 44.7M/44.7M <span style=color:#f92672>[</span>00:00&lt;00:00, 114MB/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                   Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>ResNet                                   <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>├─Conv2d: 1-1                            <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        9,408
</span></span><span style=display:flex><span>├─BatchNorm2d: 1-2                       <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>├─ReLU: 1-3                              <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        --
</span></span><span style=display:flex><span>├─MaxPool2d: 1-4                         <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>├─Sequential: 1-5                        <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-1                   <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-1                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-2             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-3                    <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-4                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-5             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-6                    <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-2                   <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-7                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-8             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-9                    <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-10                 <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-11            <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-12                   <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>├─Sequential: 1-6                        <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-3                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-13                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         73,728
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-14            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-15                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-16                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-17            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-18             <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         8,448
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-19                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-4                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-20                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-21            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-22                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-23                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-24            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-25                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>├─Sequential: 1-7                        <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-5                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-26                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         294,912
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-27            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-28                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-29                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-30            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-31             <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         33,280
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-32                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-6                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-33                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-34            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-35                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-36                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-37            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-38                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>├─Sequential: 1-8                        <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-7                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-39                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,179,648
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-40            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-41                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-42                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-43            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-44             <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           132,096
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-45                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    └─BasicBlock: 2-8                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-46                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-47            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-48                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-49                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-50            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-51                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─AdaptiveAvgPool2d: 1-9                 <span style=color:#f92672>[</span>16, 512, 1, 1<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─Linear: 1-10                           <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                513,000
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Total params: 11,689,512
</span></span><span style=display:flex><span>Trainable params: 11,689,512
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>G<span style=color:#f92672>)</span>: 29.03
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 9.63
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 635.96
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 46.76
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 692.35
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span></code></pre></div></li><li><p>Homemade <code>ResNet18</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>ResidualBlock</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#111>stride</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>BatchNorm2d</span><span style=color:#111>(</span><span style=color:#111>out_channels</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ReLU</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>BatchNorm2d</span><span style=color:#111>(</span><span style=color:#111>out_channels</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>stride</span> <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> <span style=color:#111>in_channels</span> <span style=color:#f92672>!=</span> <span style=color:#111>out_channels</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Sequential</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>                <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#111>stride</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>                <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>BatchNorm2d</span><span style=color:#111>(</span><span style=color:#111>out_channels</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>else</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Identity</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>residual</span><span style=color:#111>(</span><span style=color:#111>residual</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>ResNet18</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>64</span><span style=color:#111>,</span> <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>7</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>BatchNorm2d</span><span style=color:#111>(</span><span style=color:#ae81ff>64</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ReLU</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>maxpool</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>MaxPool2d</span><span style=color:#111>(</span><span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer1</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>_make_layer</span><span style=color:#111>(</span><span style=color:#ae81ff>64</span><span style=color:#111>,</span>  <span style=color:#ae81ff>64</span><span style=color:#111>,</span>  <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer2</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>_make_layer</span><span style=color:#111>(</span><span style=color:#ae81ff>64</span><span style=color:#111>,</span>  <span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer3</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>_make_layer</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>256</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer4</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>_make_layer</span><span style=color:#111>(</span><span style=color:#ae81ff>256</span><span style=color:#111>,</span> <span style=color:#ae81ff>512</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>avgpool</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>AdaptiveAvgPool2d</span><span style=color:#111>((</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>1</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>512</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#111>)</span><span style=color:#75715e># fully connected</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>_make_layer</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>layers</span> <span style=color:#f92672>=</span> <span style=color:#111>[]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>ResidualBlock</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>stride</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#111>layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>ResidualBlock</span><span style=color:#111>(</span><span style=color:#111>out_channels</span><span style=color:#111>,</span> <span style=color:#111>out_channels</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Sequential</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>bn1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>relu</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>maxpool</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer3</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer4</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>avgpool</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>),</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>fc</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>ResNet18</span><span style=color:#111>(),</span> <span style=color:#111>input_size</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                   Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>ResNet18                                 <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                --
</span></span><span style=display:flex><span>├─Conv2d: 1-1                            <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        9,408
</span></span><span style=display:flex><span>├─BatchNorm2d: 1-2                       <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>├─ReLU: 1-3                              <span style=color:#f92672>[</span>16, 64, 112, 112<span style=color:#f92672>]</span>        --
</span></span><span style=display:flex><span>├─MaxPool2d: 1-4                         <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>├─Sequential: 1-5                        <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-1                <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-1                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-2             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-3                    <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-4                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-5             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─Identity: 3-6                <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-7                    <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-2                <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-8                  <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-9             <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-10                   <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-11                 <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          36,864
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-12            <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>│    │    └─Identity: 3-13               <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-14                   <span style=color:#f92672>[</span>16, 64, 56, 56<span style=color:#f92672>]</span>          --
</span></span><span style=display:flex><span>├─Sequential: 1-6                        <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-3                <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-15                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         73,728
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-16            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-17                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-18                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-19            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-20             <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         8,448
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-21                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-4                <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-22                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-23            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-24                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-25                 <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         147,456
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-26            <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>│    │    └─Identity: 3-27               <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-28                   <span style=color:#f92672>[</span>16, 128, 28, 28<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>├─Sequential: 1-7                        <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-5                <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-29                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         294,912
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-30            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-31                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-32                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-33            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-34             <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         33,280
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-35                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-6                <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-36                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-37            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-38                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-39                 <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         589,824
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-40            <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         <span style=color:#ae81ff>512</span>
</span></span><span style=display:flex><span>│    │    └─Identity: 3-41               <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-42                   <span style=color:#f92672>[</span>16, 256, 14, 14<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>├─Sequential: 1-8                        <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-7                <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-43                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,179,648
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-44            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-45                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-46                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-47            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─Sequential: 3-48             <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           132,096
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-49                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    └─ResidualBlock: 2-8                <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-50                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-51            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-52                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─Conv2d: 3-53                 <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           2,359,296
</span></span><span style=display:flex><span>│    │    └─BatchNorm2d: 3-54            <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           1,024
</span></span><span style=display:flex><span>│    │    └─Identity: 3-55               <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>│    │    └─ReLU: 3-56                   <span style=color:#f92672>[</span>16, 512, 7, 7<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─AdaptiveAvgPool2d: 1-9                 <span style=color:#f92672>[</span>16, 512, 1, 1<span style=color:#f92672>]</span>           --
</span></span><span style=display:flex><span>├─Linear: 1-10                           <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                513,000
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Total params: 11,689,512
</span></span><span style=display:flex><span>Trainable params: 11,689,512
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>G<span style=color:#f92672>)</span>: 29.03
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 9.63
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 635.96
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 46.76
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 692.35
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span></code></pre></div></li></ul><h2 id=3-transformer>§3 Transformer</h2><p>Transformer is a general function fitter.</p><h3 id=31-embedding>§3.1 Embedding</h3><p>Embedding is ordered higher-dimensional representation vectors.</p><h4 id=311-nnembedding>§3.1.1 <code>nn.Embedding</code></h4><p>Words in <code>hidden_dim</code> vector space: $\vec{R} + \vec{L} = \vec{J}$, $\vec{king} - \vec{man} = \vec{queen} - \vec{woman}$.</p><p><a href=https://github.com/openai/tiktoken><code>tiktoken</code></a>, <a href=https://tiktokenizer.vercel.app/>Tiktokenizer</a></p><div class=tabset></div><ul><li><p><code>nn.Embedding</code></p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html><code>nn.Embedding</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>NUM_INDEX</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#111>EMBEDDING_DIM</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Embedding</span><span style=color:#111>(</span><span style=color:#111>NUM_INDEX</span><span style=color:#111>,</span> <span style=color:#111>EMBEDDING_DIM</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>embedding</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#f92672>.</span><span style=color:#111>detach</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>index</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>LongTensor</span><span style=color:#111>([</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>embedding</span><span style=color:#111>(</span><span style=color:#111>index</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensor<span style=color:#f92672>([[</span> 0.0378,  1.0396, -0.9673,  0.9697<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>-0.7824,  1.8141,  0.5336, -1.6396<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span> 0.1903,  0.6592,  1.4589, -0.6018<span style=color:#f92672>]])</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[</span> 0.1903,  0.6592,  1.4589, -0.6018<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span> 0.0378,  1.0396, -0.9673,  0.9697<span style=color:#f92672>]]</span>, <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;EmbeddingBackward0&gt;<span style=color:#f92672>)</span>
</span></span></code></pre></div></li><li><p><code>F.one_hot</code> then linear</p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html><code>F.one_hot</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>one_hot</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>one_hot</span><span style=color:#111>(</span><span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#f92672>=</span><span style=color:#111>NUM_INDEX</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>one_hot</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>linear</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>NUM_INDEX</span><span style=color:#111>,</span> <span style=color:#111>EMBEDDING_DIM</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>linear</span><span style=color:#f92672>.</span><span style=color:#111>weight</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Parameter</span><span style=color:#111>(</span><span style=color:#111>embedding</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#f92672>.</span><span style=color:#111>T</span><span style=color:#f92672>.</span><span style=color:#111>detach</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>linear</span><span style=color:#111>(</span><span style=color:#111>one_hot</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>()))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensor<span style=color:#f92672>([[</span>0, 0, 1<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>1, 0, 0<span style=color:#f92672>]])</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[</span> 0.1903,  0.6592,  1.4589, -0.6018<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span> 0.0378,  1.0396, -0.9673,  0.9697<span style=color:#f92672>]]</span>, <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;MmBackward0&gt;<span style=color:#f92672>)</span><span style=color:#75715e># same result</span>
</span></span></code></pre></div></li></ul><h4 id=312-sinusoidal-positional-embedding>§3.1.2 Sinusoidal Positional Embedding</h4><div class=tabset></div><ul><li><p><code>class Embedding</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Embedding</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>50257</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Embedding</span><span style=color:#111>(</span><span style=color:#111>vocab_size</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>hidden_dim</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>embedding</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span> <span style=color:#f92672>*</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>sqrt</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span></code></pre></div></li><li><p><code>class PositionalEncoding</code></p><p>The positional encoding $$\begin{aligned} PE_{(pos, 2i)} &= \sin(\frac{pos}{ 10000^{2i/{d_{model}}}}) \\ PE_{(pos, 2i + 1)} &= \cos(\frac{pos}{10000^{2i/{d_{model}}}}) \end{aligned}$$, where $pos$ is each element in the sequence up to <code>vocab_size</code>, and $d_{model}$ is <code>hidden_dim</code>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>PositionalEncoding</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>50257</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>pe</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>zeros</span><span style=color:#111>(</span><span style=color:#111>vocab_size</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>position</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>div_term</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>exp</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span> <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#111>(</span><span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#ae81ff>10000.0</span><span style=color:#111>)</span> <span style=color:#f92672>/</span> <span style=color:#111>hidden_dim</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>pe</span><span style=color:#111>[:,</span> <span style=color:#ae81ff>0</span><span style=color:#111>::</span><span style=color:#ae81ff>2</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sin</span><span style=color:#111>(</span><span style=color:#111>position</span> <span style=color:#f92672>*</span> <span style=color:#111>div_term</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>pe</span><span style=color:#111>[:,</span> <span style=color:#ae81ff>1</span><span style=color:#111>::</span><span style=color:#ae81ff>2</span><span style=color:#111>]</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cos</span><span style=color:#111>(</span><span style=color:#111>position</span> <span style=color:#f92672>*</span> <span style=color:#111>div_term</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>pe</span> <span style=color:#f92672>=</span> <span style=color:#111>pe</span><span style=color:#f92672>.</span><span style=color:#111>unsqueeze</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pe</span> <span style=color:#f92672>=</span> <span style=color:#111>pe</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>seq_length</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span> <span style=color:#f92672>+</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pe</span><span style=color:#111>[:,</span> <span style=color:#111>:</span><span style=color:#111>seq_length</span><span style=color:#111>]</span><span style=color:#f92672>.</span><span style=color:#111>requires_grad_</span><span style=color:#111>(</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span></code></pre></div></li><li><p>testing</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randint</span><span style=color:#111>(</span><span style=color:#ae81ff>50257</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>))</span><span style=color:#75715e># [batch_size, seq_length], words as int numbers</span>
</span></span><span style=display:flex><span><span style=color:#111>embeddings</span> <span style=color:#f92672>=</span> <span style=color:#111>Embedding</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>embeddings</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>positional_encoding</span> <span style=color:#f92672>=</span> <span style=color:#111>PositionalEncoding</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>positional_encoding</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><p>We will often see another way to write it:</p><div class=tabset></div><ul><li><p><code>class SinusoidalPosEmb</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>SinusoidalPosEmb</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>M</span><span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>hidden_dim</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>M</span> <span style=color:#f92672>=</span> <span style=color:#111>M</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>device</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>device</span>
</span></span><span style=display:flex><span>        <span style=color:#111>half_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        <span style=color:#111>emb</span> <span style=color:#f92672>=</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>M</span><span style=color:#111>)</span> <span style=color:#f92672>/</span> <span style=color:#111>half_dim</span>
</span></span><span style=display:flex><span>        <span style=color:#111>emb</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>exp</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#111>half_dim</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#f92672>=</span><span style=color:#111>device</span><span style=color:#111>)</span> <span style=color:#f92672>*</span> <span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#111>emb</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>emb</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#111>[</span><span style=color:#f92672>...</span><span style=color:#111>,</span> <span style=color:#00a8c8>None</span><span style=color:#111>]</span> <span style=color:#f92672>*</span> <span style=color:#111>emb</span><span style=color:#111>[</span><span style=color:#00a8c8>None</span><span style=color:#111>,</span> <span style=color:#f92672>...</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>emb</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cat</span><span style=color:#111>((</span><span style=color:#111>emb</span><span style=color:#f92672>.</span><span style=color:#111>sin</span><span style=color:#111>(),</span> <span style=color:#111>emb</span><span style=color:#f92672>.</span><span style=color:#111>cos</span><span style=color:#111>()),</span> <span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>emb</span>
</span></span></code></pre></div></li><li><p>testing</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length], words as float numbers</span>
</span></span><span style=display:flex><span><span style=color:#111>sinusoidal_pos_emb</span> <span style=color:#f92672>=</span> <span style=color:#111>SinusoidalPosEmb</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>sinusoidal_pos_emb</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><h3 id=32-transformer-encoder>§3.2 Transformer Encoder</h3><h4 id=321-ffn-mlp>§3.2.1 FFN (MLP)</h4><p><a href=https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf><em>A Neural Probabilistic Language Model</em></a></p><div class=tabset></div><ul><li><p>Equation</p><p>Feed Forward Network works on each <code>[seq_length, ]</code> vector individually $$\text{FFN}(x)=(\text{ReLU}(xW_1+b_1))W_2+b_2$$, where $\text{ReLU}(x)=\max{(0,x)}$. Here we replace <a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html><code>nn.ReLU</code></a> with <a href=https://pytorch.org/docs/stable/generated/torch.nn.GELU.html><code>nn.GELU</code></a>.</p></li><li><p><code>class FFN</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>FFN</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>in_features</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>act</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>GELU</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_features</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>act</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div></li><li><p>testing</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#ae81ff>768</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>ffn</span> <span style=color:#f92672>=</span> <span style=color:#111>FFN</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>ffn</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><h4 id=322-multiheadattention>§3.2.2 MultiheadAttention</h4><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html><code>nn.MultiheadAttention</code></a>, <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html><code>F.scaled_dot_product_attention</code></a>, <a href=https://pytorch.org/blog/flexattention/>FlexAttention</a></p><div class=tabset></div><ul><li><p>Equation</p><p>Self-Attention: Given an input $x$, we will get query $Q$, key $K$, value $V$ by $$\begin{aligned} Q&=xW^Q \\ K&=xW^K \\ V&=xW^V\end{aligned}$$Then $$\text{Attention}(Q, K, V) = \frac{1}{\sqrt{d_{k}}}\text{Softmax}(QK^\mathsf{T})V$$, where for a vector $\vec{z_i}$, $\text{Softmax}(\vec{z_i}) = \frac{e^{\vec{z_i}}}{\sum_{i=0}^N e^{\vec{z_i}}}$, and $$\text{MultiheadAttention} (Q, K, V) = \text{Concat}(\text{head}_1, \cdots, \text{head}_h) W^O$$, where $\text{head}_i = \text{Attention} (xW^Q_i, xW^K_i, xW^V_i)$, and $h$ is <code>num_heads</code> in the code.</p><p>The advantage of Softmax:</p><ul><li><a href=https://en.wikipedia.org/wiki/Matthew_effect>Matthew effect</a></li><li>Non-linearity</li><li>Normalization</li></ul><p>Note that in the figure below, only <code>q_size = k_size</code> is necessary. But in the code, <code>q_size = k_size = v_size = hidden_dim</code>.</p><p><img src=20231011-wow-it-fits-secondhand-machine-learning-attention.svg alt loading=lazy decoding=async class=full-width></p></li><li><p><code>class MultiheadAttention</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span> <span style=color:#f92672>=</span> <span style=color:#111>num_heads</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>scale</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>//</span> <span style=color:#111>num_heads</span><span style=color:#111>)</span> <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim * 3]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>permute</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span><span style=color:#75715e># [3, batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#75715e># q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># attn shape: [batch_size, num_heads, seq_length, seq_length]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>scale</span> <span style=color:#f92672>*</span> <span style=color:#111>q</span> <span style=color:#f92672>@</span> <span style=color:#111>(</span><span style=color:#111>k</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>))</span><span style=color:#75715e># `torch.matmul`</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>is_causal</span><span style=color:#111>:</span><span style=color:#75715e># masked/causal attention</span>
</span></span><span style=display:flex><span>            <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>masked_fill_</span><span style=color:#111>(</span><span style=color:#75715e># `torch.Tensor.masked_fill_`, add mask by broadcasting</span>
</span></span><span style=display:flex><span>                <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>triu</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>ones</span><span style=color:#111>((</span><span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>),</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>bool</span><span style=color:#111>),</span> <span style=color:#111>diagonal</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>                <span style=color:#111>float</span><span style=color:#111>(</span><span style=color:#d88200>&#39;-inf&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span> <span style=color:#f92672>@</span> <span style=color:#111>v</span><span style=color:#75715e># [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>reshape</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div></li><li><p>testing</p><p>Add 4 lines of <code>print()</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span> <span style=color:#f92672>=</span> <span style=color:#111>num_heads</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>scale</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>//</span> <span style=color:#111>num_heads</span><span style=color:#111>)</span> <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim * 3]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>permute</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span><span style=color:#75715e># [3, batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#75715e># q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># attn shape: [batch_size, num_heads, seq_length, seq_length]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>scale</span> <span style=color:#f92672>*</span> <span style=color:#111>q</span> <span style=color:#f92672>@</span> <span style=color:#111>(</span><span style=color:#111>k</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>))</span><span style=color:#75715e># `torch.matmul`</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>is_causal</span><span style=color:#111>:</span><span style=color:#75715e># masked/causal attention</span>
</span></span><span style=display:flex><span>            <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>masked_fill_</span><span style=color:#111>(</span><span style=color:#75715e># `torch.Tensor.masked_fill_`, add mask by broadcasting</span>
</span></span><span style=display:flex><span>                <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>triu</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>ones</span><span style=color:#111>((</span><span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>),</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>bool</span><span style=color:#111>),</span> <span style=color:#111>diagonal</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>                <span style=color:#111>float</span><span style=color:#111>(</span><span style=color:#d88200>&#39;-inf&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>attn</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>attn</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>attn</span> <span style=color:#f92672>@</span> <span style=color:#111>v</span><span style=color:#75715e># [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>reshape</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>,</span> <span style=color:#ae81ff>6</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>multihead_attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>6</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>&#39;No mask:&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>_</span> <span style=color:#f92672>=</span> <span style=color:#111>multihead_attention</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#d88200>&#39;Masked:&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>_</span> <span style=color:#f92672>=</span> <span style=color:#111>multihead_attention</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>No mask:
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[[[</span>-0.0302, -0.0241, -0.0071, -0.0822<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0041,  0.0307,  0.0372, -0.0366<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0460, -0.0571,  0.1467,  0.1020<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0685, -0.0811,  0.1513,  0.0700<span style=color:#f92672>]]</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>[[</span> 0.0744,  0.0987,  0.2944,  0.3069<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0538,  0.0855,  0.2632,  0.2898<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0052,  0.0453,  0.1585,  0.2132<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0034,  0.0774,  0.2627,  0.3394<span style=color:#f92672>]]]]</span>,
</span></span><span style=display:flex><span>       <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;UnsafeViewBackward0&gt;<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 2, 4, 4<span style=color:#f92672>])</span><span style=color:#75715e># [batch_size, num_heads, seq_length, seq_length]</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[[[</span>0.2513, 0.2529, 0.2572, 0.2386<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2487, 0.2554, 0.2571, 0.2388<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2293, 0.2268, 0.2780, 0.2659<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2282, 0.2254, 0.2843, 0.2621<span style=color:#f92672>]]</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>[[</span>0.2206, 0.2261, 0.2749, 0.2784<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2207, 0.2278, 0.2721, 0.2794<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2235, 0.2351, 0.2633, 0.2781<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2095, 0.2256, 0.2716, 0.2932<span style=color:#f92672>]]]]</span>, <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;SoftmaxBackward0&gt;<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 2, 4, 4<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>Masked:
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[[[</span>-0.0302,    -inf,    -inf,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0041,  0.0307,    -inf,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0460, -0.0571,  0.1467,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0685, -0.0811,  0.1513,  0.0700<span style=color:#f92672>]]</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>[[</span> 0.0744,    -inf,    -inf,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0538,  0.0855,    -inf,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>-0.0052,  0.0453,  0.1585,    -inf<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span> 0.0034,  0.0774,  0.2627,  0.3394<span style=color:#f92672>]]]]</span>,
</span></span><span style=display:flex><span>       <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;MaskedFillBackward0&gt;<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 2, 4, 4<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>([[[[</span>1.0000, 0.0000, 0.0000, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.4934, 0.5066, 0.0000, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.3124, 0.3089, 0.3787, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2282, 0.2254, 0.2843, 0.2621<span style=color:#f92672>]]</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         <span style=color:#f92672>[[</span>1.0000, 0.0000, 0.0000, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.4921, 0.5079, 0.0000, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.3096, 0.3257, 0.3647, 0.0000<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>[</span>0.2095, 0.2256, 0.2716, 0.2932<span style=color:#f92672>]]]]</span>, <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;SoftmaxBackward0&gt;<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 2, 4, 4<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><p>Use <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html><code>F.scaled_dot_product_attention</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span> <span style=color:#f92672>=</span> <span style=color:#111>num_heads</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>dropout</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim * 3]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>permute</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span><span style=color:#75715e># [3, batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#75715e># q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>scaled_dot_product_attention</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span><span style=color:#111>,</span> <span style=color:#111>dropout_p</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#00a8c8>if</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>training</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>0.0</span><span style=color:#111>),</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#111>is_causal</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#ae81ff>768</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>multihead_attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>multihead_attention</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div><h4 id=323-transformerencoderlayer>§3.2.3 TransformerEncoderLayer</h4><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html><code>nn.TransformerEncoderLayer</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>TransformerEncoderLayer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LayerNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LayerNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span> <span style=color:#f92672>=</span> <span style=color:#111>FFN</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>sqrt</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#111>num_layersr</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>*</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>In contrast with the Original Transformer, Layer Norm is put before Attention, see <a href=https://arxiv.org/abs/2002.04745>[2002.04745] <em>On Layer Normalization in the Transformer Architecture</em></a>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>TransformerEncoderLayer</span><span style=color:#111>(),</span> <span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#ae81ff>768</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                   Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>TransformerEncoderLayer                  <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>├─LayerNorm: 1-1                         <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             1,536
</span></span><span style=display:flex><span>├─MultiheadAttention: 1-2                <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>│    └─Linear: 2-1                       <span style=color:#f92672>[</span>1, 196, 2304<span style=color:#f92672>]</span>            1,771,776
</span></span><span style=display:flex><span>│    └─Dropout: 2-2                      <span style=color:#f92672>[</span>1, 12, 196, 196<span style=color:#f92672>]</span>         --
</span></span><span style=display:flex><span>│    └─Linear: 2-3                       <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             590,592
</span></span><span style=display:flex><span>│    └─Dropout: 2-4                      <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>├─Dropout: 1-3                           <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>├─LayerNorm: 1-4                         <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             1,536
</span></span><span style=display:flex><span>├─FFN: 1-5                               <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>│    └─Linear: 2-5                       <span style=color:#f92672>[</span>1, 196, 3072<span style=color:#f92672>]</span>            2,362,368
</span></span><span style=display:flex><span>│    └─GELU: 2-6                         <span style=color:#f92672>[</span>1, 196, 3072<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─Dropout: 2-7                      <span style=color:#f92672>[</span>1, 196, 3072<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─Linear: 2-8                       <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             2,360,064
</span></span><span style=display:flex><span>│    └─Dropout: 2-9                      <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span>├─Dropout: 1-6                           <span style=color:#f92672>[</span>1, 196, 768<span style=color:#f92672>]</span>             --
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Total params: 7,087,872
</span></span><span style=display:flex><span>Trainable params: 7,087,872
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>M<span style=color:#f92672>)</span>: 7.09
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 0.60
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 13.25
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 28.35
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 42.20
</span></span><span style=display:flex><span><span style=color:#f92672>==========================================================================================</span>
</span></span></code></pre></div><p>Most of the parameters is in <code>FNN</code> rather than <code>MultiheadAttention</code>. <code>FFN</code> takes 66.66%, while <code>MultiheadAttention</code> takes 33.33%. This is even more so with MoE, which has several FFNs.</p><h4 id=324-transformerencoder>§3.2.4 TransformerEncoder</h4><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html><code>nn.TransformerEncoder</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>TransformerEncoder</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder_layers</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ModuleList</span><span style=color:#111>([</span>
</span></span><span style=display:flex><span>            <span style=color:#111>TransformerEncoderLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>transformer_encoder_layer</span> <span style=color:#f92672>in</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder_layers</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>transformer_encoder_layer</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><h3 id=33-encoder-decoder-encoder-only-decoder-only>§3.3 Encoder-Decoder, Encoder-Only, Decoder-Only</h3><table><thead><tr><th>Encoder-Decoder: seq2seq</th><th>Encoder-Decoder: Transformer</th></tr></thead><tbody><tr><td><img src=https://d2l.ai/_images/seq2seq-state.svg alt loading=lazy decoding=async class=full-width></td><td><img src=https://d2l.ai/_images/transformer.svg alt loading=lazy decoding=async class=full-width></td></tr></tbody></table><p>| <a href=https://www.practicalai.io/understanding-transformer-model-architectures/><em>Understanding Transformer model architectures</em> (practicalai.io)</a> | <a href=https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html><em>11.9. Large-Scale Pretraining with Transformers</em> (d2l.ai)</a> |</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>NLP</th><th style=text-align:left>CV</th></tr></thead><tbody><tr><td style=text-align:left>Encoder-Decoder</td><td style=text-align:left><a href=https://arxiv.org/abs/1706.03762>[1706.03762] <em>Attention is All You Need</em></a>, <a href=https://arxiv.org/abs/1910.10683>T5</a></td><td style=text-align:left><a href=https://arxiv.org/abs/2106.08254>BEiT</a></td></tr><tr><td style=text-align:left>Encoder-Only</td><td style=text-align:left><a href=https://arxiv.org/abs/1810.04805>BERT</a>, <a href=https://github.com/AnswerDotAI/ModernBERT/>ModernBERT</a></td><td style=text-align:left><a href=https://arxiv.org/abs/2010.11929>ViT</a></td></tr><tr><td style=text-align:left>Decoder-Only</td><td style=text-align:left><a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>GPT</a>, <a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>GPT-2</a>, <a href=https://arxiv.org/abs/2005.14165>GPT-3</a>, <a href=https://github.com/karpathy/nanoGPT>nanoGPT</a>, <a href=https://github.com/KellerJordan/modded-nanogpt/>modded-nanogpt</a></td><td style=text-align:left></td></tr></tbody></table><p>Fig.1 of <a href=https://arxiv.org/abs/2304.13712>[2304.13712] <em>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</em></a>:</p><p><img src="https://pbs.twimg.com/media/GBrF-tHWYAA10wE?format=png" alt=Fig1_of_2304.13712 loading=lazy decoding=async class=full-width></p><table><thead><tr><th></th><th>Original Transformer, Encoder</th><th>Original Transformer, Decoder</th><th>ViT (Encoder-Only)</th><th>GPT (Decoder-Only)</th></tr></thead><tbody><tr><td>Self-Attention or Cross-Attention</td><td>Self-Attention</td><td>The first Self-Attention, the second Cross-Attention</td><td>Self-Attention</td><td>Self-Attention</td></tr><tr><td>Mask/Causal</td><td>✖️</td><td>✔️</td><td>✖️</td><td>✔️</td></tr></tbody></table><h3 id=34-attention-is-all-you-need-the-original-transformer>§3.4 Attention is All You Need (the Original Transformer)</h3><p><a href=https://arxiv.org/abs/1706.03762>[1706.03762] <em>Attention Is All You Need</em></a></p><ul><li>A pure Transformer structure instead of RNNs.</li><li>Use <a href=https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>Softmax</a> to let query $Q$ choose different $K^\mathsf{T}$.</li><li>The encoder provides keys $K$ and value $V$, while the decoder provides query $Q$. (Cross-Attention)</li></ul><h3 id=35-vision-transformer-vit>§3.5 Vision Transformer (ViT)</h3><p><a href=https://arxiv.org/abs/2010.11929>[2010.11929] <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a></p><ul><li>A pure Transformer structure instead of CNNs.</li><li>General function fitter instead of good inductive prior.</li><li>With enough data.</li></ul><p><img src=https://d2l.ai/_images/vit.svg alt loading=lazy decoding=async class=full-width></p><h4 id=351-patchembedding>§3.5.1 PatchEmbedding</h4><div class=tabset></div><ul><li><p><code>class PatchEmbedding</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>PatchEmbedding</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_channels</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>patch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2d</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Conv2d</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>            <span style=color:#111>in_channels</span><span style=color:#f92672>=</span><span style=color:#111>in_channels</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>            <span style=color:#111>out_channels</span><span style=color:#f92672>=</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>            <span style=color:#111>kernel_size</span><span style=color:#f92672>=</span><span style=color:#111>patch_size</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>            <span style=color:#111>stride</span><span style=color:#f92672>=</span><span style=color:#111>patch_size</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>            <span style=color:#111>padding</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>hidden_dim</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>conv2d</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div></li><li><p><code>class PatchEmbedding_noConv</code></p><p>or without convolution:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>PatchEmbedding_noConv</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>hidden_dim</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div></li></ul><h4 id=352-visiontransformer>§3.5.2 VisionTransformer</h4><div class=tabset></div><ul><li><p>Homemade <code>TransformerEncoder</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>VisionTransformer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>image_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#111>in_channels</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>patch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>        <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>patch_embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>PatchEmbedding</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>patch_size</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pos_embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Parameter</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#111>image_size</span> <span style=color:#f92672>//</span> <span style=color:#111>patch_size</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>normal_</span><span style=color:#111>(</span><span style=color:#111>std</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.02</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>class_token</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Parameter</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder</span> <span style=color:#f92672>=</span> <span style=color:#111>TransformerEncoder</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># self.layer_norm = nn.LayerNorm(hidden_dim)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>patch_embedding</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pos_embedding</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_class_token</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>class_token</span><span style=color:#f92672>.</span><span style=color:#111>expand</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cat</span><span style=color:#111>([</span><span style=color:#111>batch_class_token</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>],</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># x = self.layer_norm(x)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>[:,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>:])</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>VisionTransformer</span><span style=color:#111>(),</span><span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                        Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>VisionTransformer                             <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                151,296
</span></span><span style=display:flex><span>├─PatchEmbedding: 1-1                         <span style=color:#f92672>[</span>16, 196, 768<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-1                            <span style=color:#f92672>[</span>16, 768, 14, 14<span style=color:#f92672>]</span>         590,592
</span></span><span style=display:flex><span>├─TransformerEncoder: 1-2                     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─ModuleList: 2-2                        --                        --
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-1      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-2      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-3      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-4      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-5      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-6      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-7      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-8      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-9      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-10     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-11     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-12     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>├─Linear: 1-3                                 <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                769,000
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Total params: 86,565,352
</span></span><span style=display:flex><span>Trainable params: 86,565,352
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>G<span style=color:#f92672>)</span>: 3.23
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 9.63
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 2575.69
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 345.66
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 2930.98
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span></code></pre></div></li><li><p><code>nn.TransformerEncoder</code> (<code>torch 2.2.0+cu121</code>)</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>VisionTransformer_torch</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>image_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#111>in_channels</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>patch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>        <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>patch_embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>PatchEmbedding</span><span style=color:#111>(</span><span style=color:#111>in_channels</span><span style=color:#111>,</span> <span style=color:#111>patch_size</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pos_embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Parameter</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#111>image_size</span> <span style=color:#f92672>//</span> <span style=color:#111>patch_size</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>normal_</span><span style=color:#111>(</span><span style=color:#111>std</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.02</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>class_token</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Parameter</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>empty</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>transformer_encoder_layer</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>TransformerEncoderLayer</span><span style=color:#111>(</span><span style=color:#111>d_model</span><span style=color:#f92672>=</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>nhead</span><span style=color:#f92672>=</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>dim_feedforward</span><span style=color:#f92672>=</span><span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>batch_first</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>TransformerEncoder</span><span style=color:#111>(</span><span style=color:#111>transformer_encoder_layer</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># self.layer_norm = nn.LayerNorm(hidden_dim)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_classes</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>patch_embedding</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>pos_embedding</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>0</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_class_token</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>class_token</span><span style=color:#f92672>.</span><span style=color:#111>expand</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cat</span><span style=color:#111>([</span><span style=color:#111>batch_class_token</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>],</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_encoder</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># x = self.layer_norm(x)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>[:,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>:])</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>summary</span><span style=color:#111>(</span><span style=color:#111>VisionTransformer_torch</span><span style=color:#111>(),</span><span style=color:#111>input_size</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>,</span> <span style=color:#ae81ff>224</span><span style=color:#111>))</span>
</span></span></code></pre></div><p>will, surprisingly, get the same total parameters (<code>86,565,352</code>), though the sizes (MB) are way smaller:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Layer <span style=color:#f92672>(</span>type:depth-idx<span style=color:#f92672>)</span>                        Output Shape              Param <span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>VisionTransformer_torch                       <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                151,296
</span></span><span style=display:flex><span>├─PatchEmbedding: 1-1                         <span style=color:#f92672>[</span>16, 196, 768<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─Conv2d: 2-1                            <span style=color:#f92672>[</span>16, 768, 14, 14<span style=color:#f92672>]</span>         590,592
</span></span><span style=display:flex><span>├─TransformerEncoder: 1-2                     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            --
</span></span><span style=display:flex><span>│    └─ModuleList: 2-2                        --                        --
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-1      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-2      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-3      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-4      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-5      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-6      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-7      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-8      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-9      <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-10     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-11     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>│    │    └─TransformerEncoderLayer: 3-12     <span style=color:#f92672>[</span>16, 197, 768<span style=color:#f92672>]</span>            7,087,872
</span></span><span style=display:flex><span>├─Linear: 1-3                                 <span style=color:#f92672>[</span>16, 1000<span style=color:#f92672>]</span>                769,000
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Total params: 86,565,352
</span></span><span style=display:flex><span>Trainable params: 86,565,352
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Total mult-adds <span style=color:#f92672>(</span>G<span style=color:#f92672>)</span>: 1.86
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span><span style=display:flex><span>Input size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 9.63
</span></span><span style=display:flex><span>Forward/backward pass size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 19.40
</span></span><span style=display:flex><span>Params size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 5.44
</span></span><span style=display:flex><span>Estimated Total Size <span style=color:#f92672>(</span>MB<span style=color:#f92672>)</span>: 34.47
</span></span><span style=display:flex><span><span style=color:#f92672>===============================================================================================</span>
</span></span></code></pre></div></li></ul><h4 id=353-fine-tuning-of-vit>§3.5.3 fine-tuning of ViT</h4><p><a href=https://arxiv.org/abs/2203.09795>[2203.09795] <em>Three things everyone should know about Vision Transformers</em></a>:</p><ul><li>Parallel vision transformers.</li><li>Fine-tuning attention is all you need.</li><li>Patch preprocessing with masked self-supervised learning.</li></ul><h3 id=36-generative-pre-trained-transformer-gpt>§3.6 Generative Pre-trained Transformer (GPT)</h3><p>Note that in the original Transformer, the Decoder has two attention. However in the Decoder of GPT, there is only one attention. And GPTs are called &ldquo;Decoder-Only&rdquo; because:</p><ul><li>By using masks, GPTs are autoregressive, meaning that the model takes previous $(t-1)^{th}$ words to produce the $t^{th}$ word.</li><li>Their task is to generate text, similar to the Decoder in the original Transformer.</li></ul><h4 id=361-gptdecoderlayer>§3.6.1 GPTDecoderLayer</h4><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>GPTDecoderLayer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LayerNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LayerNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span> <span style=color:#f92672>=</span> <span style=color:#111>FFN</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>sqrt</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#111>num_layersr</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>*</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm_2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><h4 id=362-gptdecoder>§3.6.2 GPTDecoder</h4><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>GPTDecoder</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gpt_decoder_layers</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ModuleList</span><span style=color:#111>([</span>
</span></span><span style=display:flex><span>            <span style=color:#111>GPTDecoderLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>gpt_decoder_layer</span> <span style=color:#f92672>in</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gpt_decoder_layers</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>gpt_decoder_layer</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><h4 id=363-gptlanguagemodel>§3.6.3 GPTLanguageModel</h4><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>GPTLanguageModel</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>50257</span><span style=color:#111>,</span> <span style=color:#111>window_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>        <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>Embedding</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>positional_encoding</span> <span style=color:#f92672>=</span> <span style=color:#111>PositionalEncoding</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gpt_decoder</span> <span style=color:#f92672>=</span> <span style=color:#111>GPTDecoder</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>LayerNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span><span style=color:#f92672>.</span><span style=color:#111>weight</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>embedding</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#75715e># https://arxiv.org/abs/1608.05859</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>targets</span><span style=color:#f92672>=</span><span style=color:#00a8c8>None</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># index, targets shape: [batch_size, seq_length]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span> <span style=color:#f92672>=</span> <span style=color:#111>index</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># embedding</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>embedding</span><span style=color:#111>(</span><span style=color:#111>index</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>positional_encoding</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Transformer Decoder</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gpt_decoder</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># project out</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>layer_norm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>logits</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>proj</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, vocab_size]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>if</span> <span style=color:#111>targets</span> <span style=color:#f92672>is</span> <span style=color:#00a8c8>None</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#00a8c8>None</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>else</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span> <span style=color:#f92672>=</span> <span style=color:#111>logits</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>            <span style=color:#111>logits</span> <span style=color:#f92672>=</span> <span style=color:#111>logits</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#f92672>*</span><span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>vocab_size</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>targets</span> <span style=color:#f92672>=</span> <span style=color:#111>targets</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#f92672>*</span><span style=color:#111>seq_length</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>cross_entropy</span><span style=color:#111>(</span><span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>targets</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>loss</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75af00>@torch.no_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>generate</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>max_new_tokens</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># index shape [batch_size, seq_length]</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>max_new_tokens</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># crop index to the last window_size tokens</span>
</span></span><span style=display:flex><span>            <span style=color:#111>index_cond</span> <span style=color:#f92672>=</span> <span style=color:#111>index</span><span style=color:#111>[:,</span> <span style=color:#f92672>-</span><span style=color:#111>window_size</span><span style=color:#111>:]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># get the predictions</span>
</span></span><span style=display:flex><span>            <span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#111>(</span><span style=color:#111>index_cond</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># focus only on the last time step</span>
</span></span><span style=display:flex><span>            <span style=color:#111>logits</span> <span style=color:#f92672>=</span> <span style=color:#111>logits</span><span style=color:#111>[:,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>:]</span> <span style=color:#75715e># [batch_size, vocab_size]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># apply softmax to get probabilities</span>
</span></span><span style=display:flex><span>            <span style=color:#111>probs</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#75715e># [batch_size, vocab_size]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># sample from the distribution</span>
</span></span><span style=display:flex><span>            <span style=color:#111>index_next</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>multinomial</span><span style=color:#111>(</span><span style=color:#111>probs</span><span style=color:#111>,</span> <span style=color:#111>num_samples</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#75715e># [batch_size, 1]</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># append sampled index to the running sequence</span>
</span></span><span style=display:flex><span>            <span style=color:#111>index</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>cat</span><span style=color:#111>((</span><span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>index_next</span><span style=color:#111>),</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span> <span style=color:#75715e># [batch_size, seq_length+1]</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>index</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>gpt_language_model</span> <span style=color:#f92672>=</span> <span style=color:#111>GPTLanguageModel</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>index</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randint</span><span style=color:#111>(</span><span style=color:#ae81ff>50257</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>))</span><span style=color:#75715e># [batch_size, seq_length]</span>
</span></span><span style=display:flex><span><span style=color:#111>targets</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randint</span><span style=color:#111>(</span><span style=color:#ae81ff>50257</span><span style=color:#111>,</span> <span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>))</span><span style=color:#75715e># [batch_size, seq_length]</span>
</span></span><span style=display:flex><span><span style=color:#111>logits</span><span style=color:#111>,</span> <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>gpt_language_model</span><span style=color:#111>(</span><span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>targets</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>logits</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size*seq_length, vocab_size]</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>loss</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>196, 50257<span style=color:#f92672>])</span>
</span></span><span style=display:flex><span>tensor<span style=color:#f92672>(</span>10.9951, <span style=color:#111>grad_fn</span><span style=color:#f92672>=</span>&lt;NllLossBackward0&gt;<span style=color:#f92672>)</span>
</span></span></code></pre></div><h4 id=364-fine-tuning-of-llm>§3.6.4 fine-tuning of LLM</h4><p>The ULMFiT 3-step approach (see Fig.1 of <a href=https://arxiv.org/abs/1801.06146>[1801.06146] <em>Universal Language Model Fine-tuning for Text Classification</em></a>):</p><ol><li>Language Model pre-training.</li><li>Instruction tuning.</li><li>RLHF (Reinforcement Learning from Human Feedback).</li></ol><h3 id=37-variants>§3.7 Variants</h3><p>Generally speaking most papers have this kind of naming convention:</p><ul><li>Original Transformer: <a href=https://arxiv.org/abs/1706.03762>1706.03762</a></li><li>Vanilla Transformer: The original Transformer with ReLU activation and layer normalization <a href=https://arxiv.org/abs/1607.06450>1607.06450</a> outside of the residual path.</li><li>Transformer+GELU: A variant of the vanilla Transformer that uses GELU <a href=https://arxiv.org/abs/1606.08415>1606.08415</a> activations or its approximation.</li><li>Transformer++: A variant of the vanilla Transformer that uses RoPE embedding <a href=https://arxiv.org/abs/2104.09864>2104.09864</a>, RMS normalization <a href=https://arxiv.org/abs/1910.07467>1910.07467</a>, Swish activation or GeGLU activation in the FFN <a href=https://arxiv.org/abs/2002.05202>2002.05202</a>. The bias terms in all the linear layers in FFN layers and Attention layers are <code>False</code>, except for the final encoder layer. For example, <a href=https://github.com/AnswerDotAI/ModernBERT>ModernBERT</a>.</li></ul><p>Below is an implementation of Transformer++:</p><div class=tabset></div><ul><li><p><code>PyTorch</code></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>precompute_freqs_cis</span><span style=color:#111>(</span><span style=color:#111>dim</span><span style=color:#111>,</span> <span style=color:#111>end</span><span style=color:#111>,</span> <span style=color:#111>rope_theta</span><span style=color:#f92672>=</span><span style=color:#ae81ff>10000.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>freqs</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> <span style=color:#111>(</span><span style=color:#111>rope_theta</span> <span style=color:#f92672>**</span> <span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)[:</span> <span style=color:#111>(</span><span style=color:#111>dim</span> <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span><span style=color:#111>)]</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>()</span> <span style=color:#f92672>/</span> <span style=color:#111>dim</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>    <span style=color:#111>t</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>arange</span><span style=color:#111>(</span><span style=color:#111>end</span><span style=color:#111>,</span> <span style=color:#111>device</span><span style=color:#f92672>=</span><span style=color:#111>freqs</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>,</span> <span style=color:#111>dtype</span><span style=color:#f92672>=</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>float32</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>freqs</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>outer</span><span style=color:#111>(</span><span style=color:#111>t</span><span style=color:#111>,</span> <span style=color:#111>freqs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>freqs_cis</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>polar</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>ones_like</span><span style=color:#111>(</span><span style=color:#111>freqs</span><span style=color:#111>),</span> <span style=color:#111>freqs</span><span style=color:#111>)</span><span style=color:#75715e># complex64</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>freqs_cis</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>reshape_for_broadcast</span><span style=color:#111>(</span><span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>ndim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>ndim</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>assert</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>&lt;</span> <span style=color:#111>ndim</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>assert</span> <span style=color:#111>freqs_cis</span><span style=color:#f92672>.</span><span style=color:#111>shape</span> <span style=color:#f92672>==</span> <span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#ae81ff>1</span><span style=color:#111>],</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span>    <span style=color:#111>shape</span> <span style=color:#f92672>=</span> <span style=color:#111>[</span><span style=color:#111>d</span> <span style=color:#00a8c8>if</span> <span style=color:#111>i</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> <span style=color:#111>i</span> <span style=color:#f92672>==</span> <span style=color:#111>ndim</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>1</span> <span style=color:#00a8c8>for</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>d</span> <span style=color:#f92672>in</span> <span style=color:#111>enumerate</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)]</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>freqs_cis</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>apply_rotary_emb</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>q_</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>view_as_complex</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>reshape</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>q</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[:</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>],</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>    <span style=color:#111>k_</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>view_as_complex</span><span style=color:#111>(</span><span style=color:#111>k</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>reshape</span><span style=color:#111>(</span><span style=color:#f92672>*</span><span style=color:#111>k</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[:</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>],</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>    <span style=color:#111>freqs_cis</span> <span style=color:#f92672>=</span> <span style=color:#111>reshape_for_broadcast</span><span style=color:#111>(</span><span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>q_</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>q_out</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>view_as_real</span><span style=color:#111>(</span><span style=color:#111>q_</span> <span style=color:#f92672>*</span> <span style=color:#111>freqs_cis</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>flatten</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>k_out</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>view_as_real</span><span style=color:#111>(</span><span style=color:#111>k_</span> <span style=color:#f92672>*</span> <span style=color:#111>freqs_cis</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>flatten</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>q_out</span><span style=color:#f92672>.</span><span style=color:#111>type_as</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>),</span> <span style=color:#111>k_out</span><span style=color:#f92672>.</span><span style=color:#111>type_as</span><span style=color:#111>(</span><span style=color:#111>k</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>GeGLU_FFN</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>in_features</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_features</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>geglu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>assert</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>]</span> <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#111>b</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>chunk</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>a</span> <span style=color:#f92672>*</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>gelu</span><span style=color:#111>(</span><span style=color:#111>b</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>geglu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span> <span style=color:#f92672>=</span> <span style=color:#111>num_heads</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>dropout</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim * 3]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>permute</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span><span style=color:#75715e># [3, batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#75715e># q, k, v shape: [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span> <span style=color:#f92672>=</span> <span style=color:#111>apply_rotary_emb</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>q</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>),</span> <span style=color:#111>k</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>),</span> <span style=color:#111>v</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>scaled_dot_product_attention</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span><span style=color:#111>,</span> <span style=color:#111>dropout_p</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#00a8c8>if</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>training</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>0.0</span><span style=color:#111>),</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#111>is_causal</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>RMSNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>eps</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span> <span style=color:#f92672>=</span> <span style=color:#111>GeGLU_FFN</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>sqrt</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>*</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Transformer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span> <span style=color:#111>max_seq_length</span><span style=color:#f92672>=</span><span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#111>rope_theta</span><span style=color:#f92672>=</span><span style=color:#ae81ff>10000.0</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>freqs_cis</span> <span style=color:#f92672>=</span> <span style=color:#111>precompute_freqs_cis</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>//</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>max_seq_length</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>rope_theta</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ModuleList</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># self.rms_norm = nn.RMSNorm(hidden_dim, eps=1e-5)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>_</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>_</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>freqs_cis</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>freqs_cis</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>device</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>freqs_cis</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>freqs_cis</span><span style=color:#111>[:</span><span style=color:#111>seq_length</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>transformer_layer</span> <span style=color:#f92672>in</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>transformer_layer</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>freqs_cis</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># x = self.rms_norm(x)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#ae81ff>768</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>transformer</span> <span style=color:#f92672>=</span> <span style=color:#111>Transformer</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># transformer = torch.compile(transformer)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>transformer</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div></li><li><p><code>FlashAttention</code></p><p>Or, after installing:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install triton
</span></span><span style=display:flex><span>pip install flash-attn --no-build-isolation
</span></span></code></pre></div><p>We can use <a href=https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/layers/rotary.py#L341><code>flash_attn.layers.rotary.RotaryEmbedding</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>flash_attn.layers.rotary</span> <span style=color:#f92672>import</span> <span style=color:#111>RotaryEmbedding</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>GeGLU_FFN</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>in_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>in_features</span><span style=color:#111>,</span> <span style=color:#111>hidden_features</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_features</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>geglu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>assert</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>]</span> <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#111>a</span><span style=color:#111>,</span> <span style=color:#111>b</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>chunk</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>a</span> <span style=color:#f92672>*</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>gelu</span><span style=color:#111>(</span><span style=color:#111>b</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear2</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>geglu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>linear1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span> <span style=color:#f92672>=</span> <span style=color:#111>num_heads</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>dropout</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rotary_embedding</span> <span style=color:#f92672>=</span> <span style=color:#111>RotaryEmbedding</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span> <span style=color:#f92672>//</span> <span style=color:#111>num_heads</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>shape</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_qkv</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim * 3]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rotary_embedding</span><span style=color:#111>(</span><span style=color:#111>qkv</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, 3, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>qkv</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#f92672>.</span><span style=color:#111>permute</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span> <span style=color:#ae81ff>0</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>4</span><span style=color:#111>)</span><span style=color:#75715e># [3, batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>qkv</span><span style=color:#75715e># q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>scaled_dot_product_attention</span><span style=color:#111>(</span><span style=color:#111>q</span><span style=color:#111>,</span> <span style=color:#111>k</span><span style=color:#111>,</span> <span style=color:#111>v</span><span style=color:#111>,</span> <span style=color:#111>dropout_p</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#00a8c8>if</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>training</span> <span style=color:#00a8c8>else</span> <span style=color:#ae81ff>0.0</span><span style=color:#111>),</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#111>is_causal</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, num_heads, seq_length, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>transpose</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, num_heads, hidden_dim // num_heads]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#111>batch_size</span><span style=color:#111>,</span> <span style=color:#111>seq_length</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w_o</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>RMSNorm</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>eps</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span> <span style=color:#f92672>=</span> <span style=color:#111>MultiheadAttention</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span> <span style=color:#f92672>=</span> <span style=color:#111>GeGLU_FFN</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Dropout</span><span style=color:#111>(</span><span style=color:#111>dropout</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> <span style=color:#111>math</span><span style=color:#f92672>.</span><span style=color:#111>sqrt</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#111>num_layers</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attn_scale</span> <span style=color:#f92672>*</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>attention</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>residual</span> <span style=color:#f92672>=</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>rms_norm</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>ffn</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>dropout</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>+=</span> <span style=color:#111>residual</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>Transformer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>num_layers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#f92672>=</span><span style=color:#ae81ff>12</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3072</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ModuleList</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#f92672>.</span><span style=color:#111>append</span><span style=color:#111>(</span><span style=color:#111>TransformerLayer</span><span style=color:#111>(</span><span style=color:#111>num_layers</span><span style=color:#111>,</span> <span style=color:#111>num_heads</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>dropout</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># self.rms_norm = nn.RMSNorm(hidden_dim, eps=1e-5)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>transformer_layer</span> <span style=color:#f92672>in</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>transformer_layers</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>            <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>transformer_layer</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span> <span style=color:#111>is_causal</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># x = self.rms_norm(x)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>196</span><span style=color:#111>,</span> <span style=color:#ae81ff>768</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>transformer</span> <span style=color:#f92672>=</span> <span style=color:#111>Transformer</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#39;cuda&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>transformer</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>compile</span><span style=color:#111>(</span><span style=color:#111>transformer</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>transformer</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>print</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>torch.Size<span style=color:#f92672>([</span>1, 196, 768<span style=color:#f92672>])</span>
</span></span></code></pre></div></li></ul><h3 id=38-mixture-of-experts-moe>§3.8 Mixture of Experts (MoE)</h3><p>Mixture of Experts (MoE):
| <a href=https://arxiv.org/abs/1701.06538>[1701.06538] <em>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</em></a> | <a href=https://github.com/davidmrau/mixture-of-experts/>mixture-of-experts (GitHub)</a> | <a href=https://github.com/lucidrains/st-moe-pytorch>st-moe-pytorch (GitHub)</a> | <a href=https://github.com/laekov/fastmoe>FastMoE (GitHub)</a> | <a href=https://pytorch.org/blog/training-moes/><em>Training MoEs at Scale with PyTorch</em></a> |</p><p>Mixtral of Experts:
| <a href=https://arxiv.org/abs/2401.04088>[2401.04088] <em>Mixtral of Experts</em></a> | <a href=https://github.com/mistralai/mistral-src/>mistral-src (GitHub)</a> |</p><p><img src=https://raw.githubusercontent.com/mistralai/mistral-src/main/assets/smoe.png alt=MoE loading=lazy decoding=async class=full-width></p><p>FFN in the original Transformer is replaced by Mixture of Expert layer (weighted FFNs). Given $n$ experts $\lbrace{E_0, E_i, &mldr;, E_{n-1}}\rbrace$, the output of the MoE is$$\text{MoE}(x) = \sum_{i=0}^{n-1} {G(x)}_{i} \cdot E_i(x)$$where $$G(x) = \text{Softmax}(\text{TopK}(x W_g))$$By using <a href=https://pytorch.org/docs/stable/generated/torch.topk.html><code>torch.topk</code></a>, we only uses $K$ Experts, thus this model is also called Sparse Mixture of Experts (SMoE). Another benefit of experts is that we can put different experts on different GPUs, which is the similar approach of AlexNet. (It is rumored that GPT4 is using 16 experts with top2 gating. I guess Ilya Sutskever pulled the same trick again.) This usage of $\text{TopK}$ is similar to <a href=https://d2l.ai/chapter_recurrent-modern/beam-search.html>Beam Search</a> for inferencing.</p><p>In <em>Mixtral of Experts</em>, $E(x)$ is <a href=https://arxiv.org/abs/2002.05202>SwiGLU FFN</a>: $$\text{FFN}_\text{SwiGLU}(x) = (\text{Swish}_1(xW_1) \odot xV)W_2$$here we use <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.silu.html><code>F.silu</code></a>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>FFN_SwiGLU</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>14336</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>v</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>ffn_dim</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w2</span><span style=color:#111>(</span><span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>silu</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span> <span style=color:#f92672>*</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>v</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama/model.py#L250</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama/model.py#L300</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>init_weights</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>init_std</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>init</span><span style=color:#f92672>.</span><span style=color:#111>trunc_normal_</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w1</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#111>,</span> <span style=color:#111>mean</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>std</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.02</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>init</span><span style=color:#f92672>.</span><span style=color:#111>trunc_normal_</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>v</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#111>,</span> <span style=color:#111>mean</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>std</span><span style=color:#f92672>=</span><span style=color:#111>init_std</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>init</span><span style=color:#f92672>.</span><span style=color:#111>trunc_normal_</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>w2</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#111>,</span> <span style=color:#111>mean</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span><span style=color:#111>,</span> <span style=color:#111>std</span><span style=color:#f92672>=</span><span style=color:#111>init_std</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MoELayer</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>14336</span><span style=color:#111>,</span> <span style=color:#111>num_experts</span><span style=color:#f92672>=</span><span style=color:#ae81ff>8</span><span style=color:#111>,</span> <span style=color:#111>num_experts_per_tok</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>experts</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>ModuleList</span><span style=color:#111>([</span>
</span></span><span style=display:flex><span>            <span style=color:#111>FFN_SwiGLU</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>            <span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>num_experts</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>])</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gate</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#111>,</span> <span style=color:#111>num_experts</span><span style=color:#111>,</span> <span style=color:#111>bias</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_experts_per_tok</span> <span style=color:#f92672>=</span> <span style=color:#111>num_experts_per_tok</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>inputs</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#111>inputs_squashed</span> <span style=color:#f92672>=</span> <span style=color:#111>inputs</span><span style=color:#f92672>.</span><span style=color:#111>view</span><span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>inputs</span><span style=color:#f92672>.</span><span style=color:#111>shape</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#111>])</span><span style=color:#75715e># [batch_size * seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>gate_logits</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>gate</span><span style=color:#111>(</span><span style=color:#111>inputs_squashed</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size * seq_length, num_experts]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>weights</span><span style=color:#111>,</span> <span style=color:#111>selected_experts</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>topk</span><span style=color:#111>(</span><span style=color:#111>gate_logits</span><span style=color:#111>,</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>num_experts_per_tok</span><span style=color:#111>)</span><span style=color:#75715e># both [batch_size * seq_length, num_experts_per_tok]</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># print(selected_experts)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>weights</span> <span style=color:#f92672>=</span> <span style=color:#111>F</span><span style=color:#f92672>.</span><span style=color:#111>softmax</span><span style=color:#111>(</span><span style=color:#111>weights</span><span style=color:#111>,</span> <span style=color:#111>dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># iterate over each expert</span>
</span></span><span style=display:flex><span>        <span style=color:#111>results</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>zeros_like</span><span style=color:#111>(</span><span style=color:#111>inputs_squashed</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>for</span> <span style=color:#111>i</span><span style=color:#111>,</span> <span style=color:#111>expert</span> <span style=color:#f92672>in</span> <span style=color:#111>enumerate</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>experts</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>            <span style=color:#111>(</span><span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>nth_expert</span><span style=color:#111>)</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>where</span><span style=color:#111>(</span><span style=color:#111>selected_experts</span> <span style=color:#f92672>==</span> <span style=color:#111>i</span><span style=color:#111>)</span><span style=color:#75715e># both [num_index], num_index ≤ batch_size * seq_length</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># print(torch.where(selected_experts == i))</span>
</span></span><span style=display:flex><span>            <span style=color:#111>results</span><span style=color:#111>[</span><span style=color:#111>index</span><span style=color:#111>]</span> <span style=color:#f92672>+=</span> <span style=color:#111>weights</span><span style=color:#111>[</span><span style=color:#111>index</span><span style=color:#111>,</span> <span style=color:#111>nth_expert</span><span style=color:#111>,</span> <span style=color:#00a8c8>None</span><span style=color:#111>]</span> <span style=color:#f92672>*</span> <span style=color:#111>expert</span><span style=color:#111>(</span><span style=color:#111>inputs_squashed</span><span style=color:#111>[</span><span style=color:#111>index</span><span style=color:#111>])</span><span style=color:#75715e># [num_index, 1] * [num_index, hidden_dim]</span>
</span></span><span style=display:flex><span>        <span style=color:#111>results</span> <span style=color:#f92672>=</span> <span style=color:#111>results</span><span style=color:#f92672>.</span><span style=color:#111>view_as</span><span style=color:#111>(</span><span style=color:#111>inputs</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>results</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>moe_layer</span> <span style=color:#f92672>=</span> <span style=color:#111>MoELayer</span><span style=color:#111>(</span><span style=color:#111>hidden_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>8</span><span style=color:#111>,</span> <span style=color:#111>ffn_dim</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>8</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span><span style=display:flex><span><span style=color:#111>dummy</span> <span style=color:#f92672>=</span> <span style=color:#111>moe_layer</span><span style=color:#111>(</span><span style=color:#111>dummy</span><span style=color:#111>)</span><span style=color:#75715e># [batch_size, seq_length, hidden_dim]</span>
</span></span></code></pre></div><p>will get:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensor<span style=color:#f92672>([[</span>7, 3<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>7, 0<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>7, 0<span style=color:#f92672>]])</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([</span>1, 2<span style=color:#f92672>])</span>, tensor<span style=color:#f92672>([</span>1, 1<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>)</span>, tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>)</span>, tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([</span>0<span style=color:#f92672>])</span>, tensor<span style=color:#f92672>([</span>1<span style=color:#f92672>]))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>)</span>, tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>)</span>, tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>)</span>, tensor<span style=color:#f92672>([]</span>, <span style=color:#111>dtype</span><span style=color:#f92672>=</span>torch.int64<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>tensor<span style=color:#f92672>([</span>0, 1, 2<span style=color:#f92672>])</span>, tensor<span style=color:#f92672>([</span>0, 0, 0<span style=color:#f92672>]))</span>
</span></span></code></pre></div><p>There is a similar architecture called MoD (Mixture of Depth), where certain Transformer blocks are skipped by some gated mechanism. Below is Fig.1 of <a href=https://arxiv.org/abs/2404.02258>[2404.02258] <em>Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</em></a>:</p><p><img src=https://arxiv.org/html/2404.02258v1/extracted/5512346/mod.png alt=MoD loading=lazy decoding=async class=full-width></p><p>Naturally, <a href=https://github.com/epfml/llm-baselines/blob/mixture_of_depth/src/models/mod.py>Route is a <code>nn.Linear</code></a>. I find <a href=https://huggingface.co/blog/joey00072/mixture-of-depth-is-vibe>this post</a> explaining the technical difficulty we have with this architecture really well.</p><p><a href=https://arxiv.org/abs/2407.09298>[2407.09298] <em>Transformer Layers as Painters</em></a> does a lot of experiments on removing or sharing layers in Transformer:</p><blockquote><p>1.There are three distinct classes of layers (with Middle being the largest). 2. The middle layers have some degree of uniformity (but not redundancy). And 3. Execution order matters more for math and reasoning tasks than semantic tasks.</p></blockquote><h3 id=39-scaling-laws-emergence>§3.9 Scaling Laws, Emergence</h3><p>| <a href=https://arxiv.org/abs/2010.14701>[2010.14701] <em>Scaling Laws for Autoregressive Generative Modeling</em></a> | <a href=https://arxiv.org/abs/2203.15556>[2203.15556] <em>Training Compute-Optimal Large Language Models</em></a> | <a href=https://arxiv.org/abs/2304.01373>[2304.01373] <em>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</em></a> |</p><p>Previously in this article it is stated that Transformer is a general function fitter, one of the reasons is that Transformers follow scaling laws with fitting line being almost perfect.</p><p>Below is Fig.1 of <a href=https://arxiv.org/abs/2010.14701>[2010.14701] <em>Scaling Laws for Autoregressive Generative Modeling</em></a>. As we can see, the line of power law can be fitted almost perfectly. And every time I look at it I&rsquo;m amazed, you don&rsquo;t see this kind of smoothness in other NNs. RNN, for example, is really hard to train.</p><p><img src=20231011-wow-it-fits-secondhand-machine-learning-scaling-laws.png alt=scaling-law loading=lazy decoding=async class=full-width></p><p>The effect of scaling law can sometimes be misunderstood as emergence, see <a href=https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/><em>Emergent abilities and grokking: Fundamental, Mirage, or both?</em> – Windows On Theory</a>. I especially enjoy the &ldquo;jumping over a 1-meter hurdle&rdquo; analogue. Metrics of LLMs can be tricky.</p><h3 id=310-transformers-are-cnns-rnns-gnns>§3.10 Transformers are CNNs, RNNs, GNNs</h3><p>Another reason that Transformers are general function fitter is that: CNNs assume invariance of space transformation (adjacent pixels are related); RNNs assume the continuity of time series (adjacent words are related); GNNs assume the preservation of graph symmetry (a graph can be rearranged or mapped onto itself while preserving the configuration of its connections); and Transformers do not have these initial bias.</p><p>At the end of the day, we are transforming <code>[batch_size, seq_length, hidden_dim]</code> to <code>[batch_size, seq_length, hidden_dim]</code>. The intermediate steps are not important, or we happen to have found the general function fitter that is good enough for most tasks: Transformer, as I mentioned in the beginning of this chapter. See <a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html><em>The Bitter Lesson</em></a>.</p><h2 id=4-fastai>§4 <code>fastai</code></h2><p>| <a href=https://github.com/fastai/fastai>fastai (GitHub)</a> | <a href=https://docs.fast.ai/>fastai (docs)</a> | <a href=https://course.fast.ai/>Practical Deep Learning</a> |</p><img src=https://docs.fast.ai/images/layered.png width=400><h3 id=41-dataloaders>§4.1 <code>Dataloaders</code></h3><p>We did not write <a href=https://pytorch.org/tutorials/beginner/basics/data_tutorial.html><code>Datasets</code> & <code>DataLoaders</code></a>, because it&rsquo;s highly variable from tasks to tasks. In general I would suggest:</p><ol><li>Let your brain (bio-neural networks) understand the dataset intuitively by visualizing lots of examples from the dataset. (See <a href=https://karpathy.github.io/2019/04/25/recipe/><em>A Recipe for Training Neural Networks</em></a>)</li><li>Use <a href=https://github.com/pola-rs/polars><code>polars</code></a>, <a href=https://github.com/modularml/mojo><code>mojo</code></a> to load data because it&rsquo;s faster and more memory saving.</li></ol><p><a href=https://docs.fast.ai/examples/migrating_pytorch_verbose.html>Pytorch to fastai details</a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torch.utils.data</span> <span style=color:#f92672>import</span> <span style=color:#111>Dataset</span><span style=color:#111>,</span> <span style=color:#111>DataLoader</span><span style=color:#111>,</span> <span style=color:#111>SequentialSampler</span><span style=color:#111>,</span> <span style=color:#111>BatchSampler</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>fastai.vision.all</span> <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># subclass `torch.utils.data.Dataset` to create a custom Dataset</span>
</span></span><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyDataset</span><span style=color:#111>(</span><span style=color:#111>Dataset</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__len__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__getitem__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>index</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>image</span><span style=color:#111>,</span> <span style=color:#111>label</span><span style=color:#75715e># shape: image is [C, H, W], label is []</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># use `torch.utils.data` to load data</span>
</span></span><span style=display:flex><span><span style=color:#111>dataset</span> <span style=color:#f92672>=</span> <span style=color:#111>MyDataset</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>data_size</span> <span style=color:#f92672>=</span> <span style=color:#111>len</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>train_size</span> <span style=color:#f92672>=</span> <span style=color:#111>int</span><span style=color:#111>(</span><span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> <span style=color:#111>data_size</span><span style=color:#111>)</span><span style=color:#75715e># 80% is train_loader</span>
</span></span><span style=display:flex><span><span style=color:#111>indices</span> <span style=color:#f92672>=</span> <span style=color:#111>list</span><span style=color:#111>(</span><span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>data_size</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>train_indices</span> <span style=color:#f92672>=</span> <span style=color:#111>indices</span><span style=color:#111>[:</span><span style=color:#111>train_size</span><span style=color:#111>]</span>
</span></span><span style=display:flex><span><span style=color:#111>train_batch_sampler</span> <span style=color:#f92672>=</span> <span style=color:#111>BatchSampler</span><span style=color:#111>(</span><span style=color:#111>SequentialSampler</span><span style=color:#111>(</span><span style=color:#111>train_indices</span><span style=color:#111>),</span><span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>32</span><span style=color:#111>,</span><span style=color:#111>drop_last</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>train_loader</span> <span style=color:#f92672>=</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span><span style=color:#111>num_workers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>4</span><span style=color:#111>,</span><span style=color:#111>batch_sampler</span><span style=color:#f92672>=</span><span style=color:#111>train_batch_sampler</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>val_indices</span> <span style=color:#f92672>=</span> <span style=color:#111>indices</span><span style=color:#111>[</span><span style=color:#111>train_size</span><span style=color:#111>:]</span>
</span></span><span style=display:flex><span><span style=color:#111>val_batch_sampler</span> <span style=color:#f92672>=</span> <span style=color:#111>BatchSampler</span><span style=color:#111>(</span><span style=color:#111>SequentialSampler</span><span style=color:#111>(</span><span style=color:#111>val_indices</span><span style=color:#111>),</span><span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#ae81ff>32</span><span style=color:#111>,</span><span style=color:#111>drop_last</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>val_loader</span> <span style=color:#f92672>=</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span><span style=color:#111>num_workers</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#111>batch_sampler</span><span style=color:#f92672>=</span><span style=color:#111>val_batch_sampler</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># use `fastai.vision.all.DataLoaders` to combine training data and validation data</span>
</span></span><span style=display:flex><span><span style=color:#111>dls</span> <span style=color:#f92672>=</span> <span style=color:#111>DataLoaders</span><span style=color:#111>(</span><span style=color:#111>train_loader</span><span style=color:#111>,</span> <span style=color:#111>val_loader</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>Or you can use <a href=https://docs.fast.ai/tutorial.datablock.html><code>DataBlock</code></a>.</p><h3 id=42-learner>§4.2 <code>Learner</code></h3><p>Load the model:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>MyModel</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>cuda</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Use <code>fastai.vision.all.OptimWrapper</code> to wrap <a href=https://arxiv.org/abs/1711.05101><code>AdamW</code></a> optimizer:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>def</span> <span style=color:#75af00>WrapperAdamW</span><span style=color:#111>(</span><span style=color:#111>param_groups</span><span style=color:#111>,</span><span style=color:#f92672>**</span><span style=color:#111>kwargs</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>return</span> <span style=color:#111>OptimWrapper</span><span style=color:#111>(</span><span style=color:#111>param_groups</span><span style=color:#111>,</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>AdamW</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><a href=https://docs.fast.ai/learner.html><code>Learner</code></a>, <a href=https://docs.fast.ai/callback.fp16.html#learner.to_fp16><code>Learner.to_fp16</code></a>, <a href=https://docs.fast.ai/callback.core.html><code>Callbacks</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>functools</span> <span style=color:#f92672>import</span> <span style=color:#111>partial</span><span style=color:#75715e># python standard library</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span> <span style=color:#f92672>=</span> <span style=color:#111>Learner</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>    <span style=color:#111>dls</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>model</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>path</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;custom_path&#39;</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss_func</span><span style=color:#f92672>=</span><span style=color:#111>custom_loss</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>metrics</span><span style=color:#f92672>=</span><span style=color:#111>[</span><span style=color:#111>custom_metric</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>    <span style=color:#111>opt_func</span><span style=color:#f92672>=</span><span style=color:#111>partial</span><span style=color:#111>(</span><span style=color:#111>WrapperAdamW</span><span style=color:#111>,</span><span style=color:#111>eps</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># opt_func=partial(OptimWrapper,opt=torch.optim.AdamW,eps=1e-7)</span>
</span></span><span style=display:flex><span>    <span style=color:#111>cbs</span><span style=color:#f92672>=</span><span style=color:#111>[</span>
</span></span><span style=display:flex><span>        <span style=color:#111>CSVLogger</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>        <span style=color:#111>GradientClip</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>        <span style=color:#111>EMACallback</span><span style=color:#111>(),</span>
</span></span><span style=display:flex><span>        <span style=color:#111>SaveModelCallback</span><span style=color:#111>(</span><span style=color:#111>monitor</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;custom_metric&#39;</span><span style=color:#111>,</span><span style=color:#111>comp</span><span style=color:#f92672>=</span><span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>less</span><span style=color:#111>,</span><span style=color:#111>every_epoch</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>),</span>
</span></span><span style=display:flex><span>        <span style=color:#111>GradientAccumulation</span><span style=color:#111>(</span><span style=color:#111>n_acc</span><span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span><span style=color:#f92672>//</span><span style=color:#ae81ff>32</span><span style=color:#111>)</span><span style=color:#75715e># divided by `batch_size`</span>
</span></span><span style=display:flex><span>    <span style=color:#111>]</span>
</span></span><span style=display:flex><span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>to_fp16</span><span style=color:#111>()</span>
</span></span></code></pre></div><p><a href=https://docs.fast.ai/callback.schedule.html#learner.lr_find><code>Learner.lr_find</code></a>, <a href=https://arxiv.org/abs/1506.01186>[1506.01186] <em>Cyclical Learning Rates for Training Neural Networks</em></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>lr_find</span><span style=color:#111>(</span><span style=color:#111>suggest_funcs</span><span style=color:#f92672>=</span><span style=color:#111>(</span><span style=color:#111>slide</span><span style=color:#111>,</span> <span style=color:#111>valley</span><span style=color:#111>))</span>
</span></span></code></pre></div><p><a href=https://docs.fast.ai/callback.schedule.html#learner.fit_one_cycle><code>Learner.fit_one_cycle</code></a> uses <a href=https://arxiv.org/abs/1708.07120>1cycle policy</a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>fit_one_cycle</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>8</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>lr_max</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-5</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>wd</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>pct_start</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>div</span><span style=color:#f92672>=</span><span style=color:#ae81ff>25</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>div_final</span><span style=color:#f92672>=</span><span style=color:#ae81ff>100000</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span><span style=color:#111>)</span>
</span></span></code></pre></div><p><a href=https://docs.fast.ai/learner.html#learner.save><code>Learner.save</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>save</span><span style=color:#111>(</span><span style=color:#d88200>&#34;my_model_opt&#34;</span><span style=color:#111>,</span> <span style=color:#111>with_opt</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>save</span><span style=color:#111>(</span><span style=color:#d88200>&#34;my_model&#34;</span><span style=color:#111>,</span> <span style=color:#111>with_opt</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span></code></pre></div><p><a href=https://fastxtend.benjaminwarner.dev/callback.compiler.html><code>Learner.compile</code></a>, <a href=https://pytorch.org/docs/master/compile/get-started.html><code>torch.compile</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>fastxtend.callback</span> <span style=color:#f92672>import</span> <span style=color:#111>compiler</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>Learner</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>compile</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># or</span>
</span></span><span style=display:flex><span><span style=color:#111>Learner</span><span style=color:#111>(</span><span style=color:#f92672>...</span><span style=color:#111>,</span> <span style=color:#111>cbs</span><span style=color:#f92672>=</span><span style=color:#111>CompilerCallback</span><span style=color:#111>())</span>
</span></span></code></pre></div><h2 id=5-transfer-learning>§5 Transfer Learning</h2><p>For different dataset and different goals.</p><h3 id=51-load-pretrained-resnet-vit>§5.1 Load Pretrained ResNet, ViT</h3><p>| <a href=https://www.kaggle.com/code/csaroff/which-timm-models-are-best-2023-11-29/notebook>Which Timm Models Are Best 2023-11-29 | Kaggle</a> |</p><div class=tabset></div><ul><li><p><code>ResNet101</code></p><p><a href=https://docs.fast.ai/vision.learner.html#vision_learner><code>fastai.vision.all.vision_learner</code></a></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>fastai.vision.all</span> <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span><span style=color:#75715e># https://github.com/pytorch/vision/tree/main/torchvision/models</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision.models</span> <span style=color:#f92672>import</span> <span style=color:#111>resnet101</span>
</span></span><span style=display:flex><span><span style=color:#75715e># https://pytorch.org/vision/stable/models.html</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision.models</span> <span style=color:#f92672>import</span> <span style=color:#111>ResNet101_Weights</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dls</span> <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span> <span style=color:#f92672>=</span> <span style=color:#111>vision_learner</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>    <span style=color:#111>dls</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>resnet101</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>pretrained</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>weights</span><span style=color:#f92672>=</span><span style=color:#111>ResNet101_Weights</span><span style=color:#f92672>.</span><span style=color:#111>IMAGENET1K_V2</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>    <span style=color:#111>metrics</span><span style=color:#f92672>=</span><span style=color:#111>error_rate</span>
</span></span><span style=display:flex><span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>fine_tune</span><span style=color:#111>(</span>
</span></span><span style=display:flex><span>    <span style=color:#111>freeze_epochs</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#75715e># freeze_epochs run first</span>
</span></span><span style=display:flex><span>    <span style=color:#111>epochs</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>save</span><span style=color:#111>(</span><span style=color:#d88200>&#34;finetuned_resnet101&#34;</span><span style=color:#111>,</span> <span style=color:#111>with_opt</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span></code></pre></div></li><li><p><code>ViT_B_16</code></p><p><a href=https://docs.fast.ai/learner.html><code>fastai.vision.all.Learner</code></a>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>fastai.vision.all</span> <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision.models</span> <span style=color:#f92672>import</span> <span style=color:#111>vit_b_16</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torchvision.models</span> <span style=color:#f92672>import</span> <span style=color:#111>ViT_B_16_Weights</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dls</span> <span style=color:#f92672>=</span> <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># https://github.com/rasbt/ViT-finetuning-scripts/</span>
</span></span><span style=display:flex><span><span style=color:#111>model</span> <span style=color:#f92672>=</span> <span style=color:#111>vit_b_16</span><span style=color:#111>(</span><span style=color:#111>weights</span><span style=color:#f92672>=</span><span style=color:#111>ViT_B_16_Weights</span><span style=color:#f92672>.</span><span style=color:#111>IMAGENET1K_V1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>heads</span><span style=color:#f92672>.</span><span style=color:#111>head</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#111>in_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>768</span><span style=color:#111>,</span> <span style=color:#111>out_features</span><span style=color:#f92672>=</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span><span style=color:#75715e># replace projection layer</span>
</span></span><span style=display:flex><span><span style=color:#111>model</span><span style=color:#f92672>.</span><span style=color:#111>to</span><span style=color:#111>(</span><span style=color:#d88200>&#34;cuda&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span> <span style=color:#f92672>=</span> <span style=color:#111>Learner</span><span style=color:#111>(</span><span style=color:#111>dls</span><span style=color:#111>,</span> <span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>metrics</span><span style=color:#f92672>=</span><span style=color:#111>error_rate</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>fine_tune</span><span style=color:#111>(</span><span style=color:#111>freeze_epochs</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span> <span style=color:#111>epochs</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>learn</span><span style=color:#f92672>.</span><span style=color:#111>save</span><span style=color:#111>(</span><span style=color:#d88200>&#34;finetuned_vit_b_16&#34;</span><span style=color:#111>,</span> <span style=color:#111>with_opt</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>)</span>
</span></span></code></pre></div></li></ul><h3 id=52-acousticgravitational-wave-classification>§5.2 Acoustic/Gravitational Wave Classification</h3><p><a href=https://arxiv.org/abs/1912.11370>[1912.11370] <em>Big Transfer (BiT): General Visual Representation Learning</em></a>:</p><blockquote><p>We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT).</p></blockquote><h4 id=521-acoustic-wave-classification>§5.2.1 Acoustic Wave Classification</h4><ul><li><a href=https://etown.medium.com/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52><em>Great results on audio classification with fastai library</em> | by Ethan Sutin</a></li><li><a href=https://github.com/etown/dl1/blob/master/UrbanSoundClassification.ipynb><code>UrbanSoundClassification.ipynb</code> (GitHub)</a></li></ul><p>Each subfigure of the figure below is a <em>Power Spectrum</em>:</p><ul><li>The horizontal axis is <em>Time</em> ($\text{s}$).</li><li>The vertical axis is <em>Frequency</em> ($\text{Hz}$) of the vibration.</li><li>The color (from dark to red to white) is <em>Sound Intensity Level</em> ($\text{dB}$):</li></ul><p><img src=https://miro.medium.com/v2/resize:fit:1100/format:webp/1*D_yXVrrJD1Y46Z1T0OTOVA.png alt loading=lazy decoding=async class=full-width></p><p>Use <a href=https://librosa.org/doc/latest/generated/librosa.display.specshow.html#librosa.display.specshow><code>librosa.display.specshow</code></a> to draw <em>Power Spectrum</em>, then save as <code>.png</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>S</span> <span style=color:#f92672>=</span> <span style=color:#111>librosa</span><span style=color:#f92672>.</span><span style=color:#111>feature</span><span style=color:#f92672>.</span><span style=color:#111>melspectrogram</span><span style=color:#111>(</span><span style=color:#111>y</span><span style=color:#f92672>=</span><span style=color:#111>samples</span><span style=color:#111>,</span> <span style=color:#111>sr</span><span style=color:#f92672>=</span><span style=color:#111>sample_rate</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>librosa</span><span style=color:#f92672>.</span><span style=color:#111>display</span><span style=color:#f92672>.</span><span style=color:#111>specshow</span><span style=color:#111>(</span><span style=color:#111>librosa</span><span style=color:#f92672>.</span><span style=color:#111>power_to_db</span><span style=color:#111>(</span><span style=color:#111>S</span><span style=color:#111>,</span> <span style=color:#111>ref</span><span style=color:#f92672>=</span><span style=color:#111>np</span><span style=color:#f92672>.</span><span style=color:#111>max</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>filename</span>  <span style=color:#f92672>=</span> <span style=color:#111>spectrogram_path</span><span style=color:#f92672>/</span><span style=color:#111>fold</span><span style=color:#f92672>/</span><span style=color:#111>Path</span><span style=color:#111>(</span><span style=color:#111>audio_file</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>name</span><span style=color:#f92672>.</span><span style=color:#111>replace</span><span style=color:#111>(</span><span style=color:#d88200>&#39;.wav&#39;</span><span style=color:#111>,</span><span style=color:#d88200>&#39;.png&#39;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>plt</span><span style=color:#f92672>.</span><span style=color:#111>savefig</span><span style=color:#111>(</span><span style=color:#111>filename</span><span style=color:#111>,</span> <span style=color:#111>dpi</span><span style=color:#f92672>=</span><span style=color:#ae81ff>400</span><span style=color:#111>,</span> <span style=color:#111>bbox_inches</span><span style=color:#f92672>=</span><span style=color:#d88200>&#39;tight&#39;</span><span style=color:#111>,</span><span style=color:#111>pad_inches</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>Use <code>fastai</code> to load pretrained model <code>ResNet34</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>learn</span> <span style=color:#f92672>=</span> <span style=color:#111>cnn_learner</span><span style=color:#111>(</span><span style=color:#111>data</span><span style=color:#111>,</span> <span style=color:#111>models</span><span style=color:#f92672>.</span><span style=color:#111>resnet34</span><span style=color:#111>,</span> <span style=color:#111>metrics</span><span style=color:#f92672>=</span><span style=color:#111>error_rate</span><span style=color:#111>)</span>
</span></span></code></pre></div><h4 id=522-gravitational-wave-classification>§5.2.2 Gravitational Wave Classification</h4><ul><li><a href=https://arxiv.org/abs/2303.13917>[2303.13917] <em>Convolutional Neural Networks for the classification of glitches in gravitational-wave data streams</em></a></li></ul><p>The picture below is Fig.2 of the paper:</p><p><img src=20231011-wow-it-fits-secondhand-machine-learning-05_2303.13917_GWs.png alt loading=lazy decoding=async class=full-width></p><p>Use <code>fastai</code> to load pretrained model <code>ResNet18</code>, <code>ResNet26</code>, <code>ResNet34</code>, <code>ResNet50</code>, <code>ConvNext_Nano</code>, <code>ConvNext_Tiny</code>.</p><h3 id=53-category-unknown-confidence-level>§5.3 Category &ldquo;Unknown&rdquo;, Confidence Level</h3><ul><li>Choose the size of the model by the size of the dataset, to avoid overfitting.</li><li>When the output probability of the final MLP layer is not dominated by one category (less than 95% or some threshold), you should be extra careful. Because actually this prediction is not correct. There is an entire research field on predicting the confidence level of a prediction, see <a href=https://arxiv.org/abs/2103.15718>[2103.15718] <em>von Mises-Fisher Loss: An Exploration of Embedding Geometries for Supervised Learning</em></a>, <a href=https://arxiv.org/abs/2107.03342>[2107.03342] <em>A Survey of Uncertainty in Deep Neural Networks</em></a>, <a href=https://arxiv.org/abs/1706.04599>[1706.04599] <em>On Calibration of Modern Neural Networks</em></a>.</li></ul><div class=post-date><span class="g time">October 11, 2023</span> &#8729;
<a href=https://ChenLi2049.github.io/tags/programming/>programming</a></div></section><div id=comments><script src=https://utteranc.es/client.js repo=ChenLi2049/ChenLi2049.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></main></body></html>