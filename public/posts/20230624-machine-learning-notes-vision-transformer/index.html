<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - https://chenlinear.github.io">
    <title>Machine Learning Notes: Vision Transformer (ViT) | Chen Li</title>
    <meta name="description" content="Chen Li&#39;s personal blog">
    <meta property="og:title" content="Machine Learning Notes: Vision Transformer (ViT)" />
<meta property="og:description" content="(Please refer to Wow It Fits! — Secondhand Machine Learning. There are a lot of pictures in this post so it might take a while to load.)
Original paper is [2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Here&rsquo;s some notes from 08. PyTorch Paper Replicating - Zero to Mastery Learn PyTorch for Deep Learning.
The relation between this structure and the equations:
First import packages:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenlinear.github.io/posts/20230624-machine-learning-notes-vision-transformer/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-06-24T00:00:00+00:00" />

    <meta itemprop="name" content="Machine Learning Notes: Vision Transformer (ViT)">
<meta itemprop="description" content="(Please refer to Wow It Fits! — Secondhand Machine Learning. There are a lot of pictures in this post so it might take a while to load.)
Original paper is [2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Here&rsquo;s some notes from 08. PyTorch Paper Replicating - Zero to Mastery Learn PyTorch for Deep Learning.
The relation between this structure and the equations:
First import packages:"><meta itemprop="datePublished" content="2023-06-24T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-06-24T00:00:00+00:00" />
<meta itemprop="wordCount" content="2213">
<meta itemprop="keywords" content="programming," />
    
    <link rel="canonical" href="https://chenlinear.github.io/posts/20230624-machine-learning-notes-vision-transformer/">
    <link rel="icon" href="https://chenlinear.github.io/assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Chen Li" href="https://chenlinear.github.io/atom.xml" />
    <link rel="alternate" type="application/json" title="Chen Li" href="https://chenlinear.github.io/feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 '-apple-system',BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-align:justify;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:rbg(169,169,169,1);color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}h1,h2,h3,h4,h5{font-size:20px;font-weight:600;text-align:center}strong,b{font-size:inherit;font-weight:600}header{line-height:2;padding-bottom:1.5rem}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a:hover,a.heading-link{text-decoration:none}a,a:visited{color:#008b8b}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none}ul{list-style-type:circle}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:90ch;margin:0 auto}header{line-height:1;display:flex;justify-content:space-between;padding-bottom:1rem}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:135px;width:135px;position:relative;margin:0 0 0 15px;float:right;border-radius:25%} table{border-collapse:collapse}table th,table td{border:1px solid #bebebe;padding:0 5px}.toc{margin:0 auto;width:100%;padding:0;margin-bottom:10px;background-color:#f9f9f9}</style>
  
    
  
  
  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "BlogPosting",
      "articleSection": "posts",
      "name": "Machine Learning Notes: Vision Transformer (ViT)",
      "headline": "Machine Learning Notes: Vision Transformer (ViT)",
      "alternativeHeadline": "",
      "description": "(Please refer to Wow It Fits! — Secondhand Machine Learning. There are a lot of pictures in this post so it might take a while to load.)\nOriginal paper is [2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Here\u0026rsquo;s some notes from 08. PyTorch Paper Replicating - Zero to Mastery Learn PyTorch for Deep Learning.\nThe relation between this structure and the equations:\nFirst import packages:",
      "inLanguage": "en-us",
      "isFamilyFriendly": "true",
      "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https:\/\/chenlinear.github.io\/posts\/20230624-machine-learning-notes-vision-transformer\/"
      },
      "author" : {
          "@type": "Person",
          "name": "Chen Li"
      },
      "creator" : {
          "@type": "Person",
          "name": "Chen Li"
      },
      "accountablePerson" : {
          "@type": "Person",
          "name": "Chen Li"
      },
      "copyrightHolder" : "Chen Li",
      "copyrightYear" : "2023",
      "dateCreated": "2023-06-24T00:00:00.00Z",
      "datePublished": "2023-06-24T00:00:00.00Z",
      "dateModified": "2023-06-24T00:00:00.00Z",
      "publisher":{
          "@type":"Organization",
          "name": "Chen Li",
          "url": "https://chenlinear.github.io",
          "logo": {
              "@type": "ImageObject",
              "url": "https:\/\/chenlinear.github.io\/assets\/favicon.ico",
              "width":"32",
              "height":"32"
          }
      },
      "image": "https://chenlinear.github.io/assets/favicon.ico",
      "url" : "https:\/\/chenlinear.github.io\/posts\/20230624-machine-learning-notes-vision-transformer\/",
      "wordCount" : "2213",
      "genre" : [ "programming" ],
      "keywords" : [ "programming" ]
  }
  </script>
  

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/katex.min.css">
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/katex.min.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.6/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    <header>
<p style="padding: 0;margin: 0;">
  <a href="/">
    <b>Chen Li</b>
    <span class="text-stone-500 animate-blink"></span>
  </a>
</p>
<ul style="padding: 0;margin: 0;">
  
  
  <li class="">
    <a href="/cv/"><span>CV</span></a>
    
  <li class="">
    <a href="/posts/"><span>Posts</span></a>
    
  <li class="">
    <a href="/about/"><span>About</span></a>
    
  </li>
</ul>
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post">Machine Learning Notes: Vision Transformer (ViT)</h2>
      
      <div class="toc">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#1-embedding">§1 Embedding</a>
      <ul>
        <li><a href="#11-patch-embedding">§1.1 Patch Embedding</a></li>
        <li><a href="#12-class-token-embedding-position-embedding-and-put-them-together">§1.2 Class Token Embedding, Position Embedding and Put them together</a></li>
      </ul>
    </li>
    <li><a href="#2-multi-head-self-attention-msa">§2 Multi-Head Self Attention (MSA)</a></li>
    <li><a href="#3-multilayer-perception-mlp">§3 Multilayer Perception (MLP)</a></li>
    <li><a href="#4-transformer-encoder">§4 Transformer Encoder</a>
      <ul>
        <li><a href="#41-put-blocks-above-together">§4.1 Put Blocks Above Together</a></li>
        <li><a href="#42-build-encoder-with-torchnntransformerencoderlayerhttpspytorchorgdocsstablegeneratedtorchnntransformerencoderlayerhtmltorchnntransformerencoderlayer">§4.2 Build Encoder with <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer"><code>torch.nn.TransformerEncoderLayer()</code></a>:</a></li>
      </ul>
    </li>
    <li><a href="#5-vit">§5 ViT</a></li>
    <li><a href="#6-optimizer-loss-function">§6 Optimizer, Loss Function</a></li>
    <li><a href="#7-use-pretrained-model-transfer-learning">§7 Use Pretrained model (Transfer Learning)</a></li>
    <li><a href="#8-cnn-or-transformer">§8 CNN or Transformer</a></li>
  </ul>
</nav>
      </div>
      
      <p>(Please refer to <a href="https://chenlinear.github.io/posts/20231011-wow-it-fits-secondhand-machine-learning/"><em>Wow It Fits! — Secondhand Machine Learning</em></a>. There are a lot of pictures in this post so it might take a while to load.)</p>
<p>Original paper is <a href="https://arxiv.org/abs/2010.11929">[2010.11929] <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a>. Here&rsquo;s some notes from <a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/"><em>08. PyTorch Paper Replicating</em> - Zero to Mastery Learn PyTorch for Deep Learning</a>.</p>
<p>The relation between this structure and the equations:</p>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-mapping-the-four-equations-to-figure-1.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<p>First import packages:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch.nn</span> <span style="color:#00a8c8">as</span> <span style="color:#111">nn</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch.nn.functional</span> <span style="color:#00a8c8">as</span> <span style="color:#111">F</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> <span style="color:#111">torchinfo</span> <span style="color:#f92672">import</span> <span style="color:#111">summary</span>
</span></span></code></pre></div><h2 id="1-embedding">§1 Embedding</h2>
<p>This section is based on Eq. 1.</p>
<h3 id="11-patch-embedding">§1.1 Patch Embedding</h3>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-replicating-the-patch-embedding-layer.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<p>For $\mathbf{x} \in \mathbb{R}^{H \times W \times C} \rightarrow \mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$, where $H$ is the height of the image, $W$ is the width of the image, $C$ is the color channel of the image (in this case the image is RGB, thus $C=3$), number of patches $N=H W / P^{2}$:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Create a class which subclasses nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">PatchEmbedding</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#d88200">&#34;&#34;&#34;Turns a 2D input image into a 1D sequence learnable embedding vector.
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">    
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">        in_channels (int): Number of color channels for the input images. Defaults to 3.
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">        patch_size (int): Size of patches to convert input image into. Defaults to 16.
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">    &#34;&#34;&#34;</span> 
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. Initialize the class with appropriate variables</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> 
</span></span><span style="display:flex;"><span>                 <span style="color:#111">in_channels</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">patch_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dim</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. Create a layer to turn an image into patches</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">patcher</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Conv2d</span><span style="color:#111">(</span><span style="color:#111">in_channels</span><span style="color:#f92672">=</span><span style="color:#111">in_channels</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                 <span style="color:#111">out_channels</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                 <span style="color:#111">kernel_size</span><span style="color:#f92672">=</span><span style="color:#111">patch_size</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                 <span style="color:#111">stride</span><span style="color:#f92672">=</span><span style="color:#111">patch_size</span><span style="color:#111">,</span><span style="color:#75715e"># which means stride = kernel_size</span>
</span></span><span style="display:flex;"><span>                                 <span style="color:#111">padding</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 4. Create a layer to flatten the patch feature maps into a single dimension</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">flatten</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Flatten</span><span style="color:#111">(</span><span style="color:#111">start_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">2</span><span style="color:#111">,</span> <span style="color:#75715e"># only flatten the feature map dimensions into a single vector</span>
</span></span><span style="display:flex;"><span>                                  <span style="color:#111">end_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. Define the forward method </span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Create assertion to check that inputs are the correct shape</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># image_resolution = x.shape[-1]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># assert image_resolution % patch_size == 0, f&#34;Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Perform the forward pass</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x_patched</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">patcher</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x_flattened</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">flatten</span><span style="color:#111">(</span><span style="color:#111">x_patched</span><span style="color:#111">)</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 6. Make sure the output shape has the right order </span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">x_flattened</span><span style="color:#f92672">.</span><span style="color:#111">permute</span><span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#ae81ff">2</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">)</span> <span style="color:#75715e"># adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span>
</span></span></code></pre></div><p>To use <code>PatchEmbedding</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">dummy</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Input image with batch dimension shape: </span><span style="color:#d88200">{</span><span style="color:#111">dummy</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span><span style="color:#75715e"># [batch_size, C, W, H]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create an instance of class PatchEmbedding</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patchify</span> <span style="color:#f92672">=</span> <span style="color:#111">PatchEmbedding</span><span style="color:#111">(</span><span style="color:#111">in_channels</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#111">patch_size</span><span style="color:#f92672">=</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Patching embedding shape: </span><span style="color:#d88200">{</span><span style="color:#111">patchify</span><span style="color:#111">(</span><span style="color:#111">dummy</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span><span style="color:#75715e"># [batch_size, N, P^2•C]</span>
</span></span></code></pre></div><p>will get:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Input image with batch dimension shape: torch.Size<span style="color:#f92672">([</span>1, 3, 224, 224<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Patching embedding shape: torch.Size<span style="color:#f92672">([</span>1, 196, 768<span style="color:#f92672">])</span>
</span></span></code></pre></div><h3 id="12-class-token-embedding-position-embedding-and-put-them-together">§1.2 Class Token Embedding, Position Embedding and Put them together</h3>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-putting-it-all-together.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">image</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Set patch size</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patch_size</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Print shape of original image tensor and get the image dimensions</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Image tensor shape: </span><span style="color:#d88200">{</span><span style="color:#111">image</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">height</span><span style="color:#111">,</span> <span style="color:#111">width</span> <span style="color:#f92672">=</span> <span style="color:#111">image</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#ae81ff">1</span><span style="color:#111">],</span> <span style="color:#111">image</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#ae81ff">2</span><span style="color:#111">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Get image tensor and add batch dimension</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">image</span><span style="color:#f92672">.</span><span style="color:#111">unsqueeze</span><span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Input image with batch dimension shape: </span><span style="color:#d88200">{</span><span style="color:#111">x</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Create patch embedding layer</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patch_embedding_layer</span> <span style="color:#f92672">=</span> <span style="color:#111">PatchEmbedding</span><span style="color:#111">(</span><span style="color:#111">in_channels</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                       <span style="color:#111">patch_size</span><span style="color:#f92672">=</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                       <span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. Pass image through patch embedding layer</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patch_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">patch_embedding_layer</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Patching embedding shape: </span><span style="color:#d88200">{</span><span style="color:#111">patch_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 6. Create class token embedding</span>
</span></span><span style="display:flex;"><span><span style="color:#111">batch_size</span> <span style="color:#f92672">=</span> <span style="color:#111">patch_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#111">]</span>
</span></span><span style="display:flex;"><span><span style="color:#111">embedding_dimension</span> <span style="color:#f92672">=</span> <span style="color:#111">patch_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">]</span>
</span></span><span style="display:flex;"><span><span style="color:#111">class_token</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Parameter</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">ones</span><span style="color:#111">(</span><span style="color:#111">batch_size</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">embedding_dimension</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>                           <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e"># make sure it&#39;s learnable</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Class token embedding shape: </span><span style="color:#d88200">{</span><span style="color:#111">class_token</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 7. Prepend class token embedding to patch embedding</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patch_embedding_class_token</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">cat</span><span style="color:#111">((</span><span style="color:#111">class_token</span><span style="color:#111">,</span> <span style="color:#111">patch_embedding</span><span style="color:#111">),</span> <span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Patch embedding with class token shape: </span><span style="color:#d88200">{</span><span style="color:#111">patch_embedding_class_token</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 8. Create position embedding</span>
</span></span><span style="display:flex;"><span><span style="color:#111">number_of_patches</span> <span style="color:#f92672">=</span> <span style="color:#111">int</span><span style="color:#111">((</span><span style="color:#111">height</span> <span style="color:#f92672">*</span> <span style="color:#111">width</span><span style="color:#111">)</span> <span style="color:#f92672">/</span> <span style="color:#111">patch_size</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">position_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Parameter</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">ones</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">number_of_patches</span><span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">embedding_dimension</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>                                  <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e"># make sure it&#39;s learnable</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Position embedding shape: </span><span style="color:#d88200">{</span><span style="color:#111">position_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 9. Add position embedding to patch embedding with class token</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patch_and_position_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">patch_embedding_class_token</span> <span style="color:#f92672">+</span> <span style="color:#111">position_embedding</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Patch and position embedding shape: </span><span style="color:#d88200">{</span><span style="color:#111">patch_and_position_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>will get:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Image tensor shape: torch.Size<span style="color:#f92672">([</span>3, 224, 224<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Input image with batch dimension shape: torch.Size<span style="color:#f92672">([</span>1, 3, 224, 224<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Patching embedding shape: torch.Size<span style="color:#f92672">([</span>1, 196, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Class token embedding shape: torch.Size<span style="color:#f92672">([</span>1, 1, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Patch embedding with class token shape: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Position embedding shape: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Patch and position embedding shape: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span></code></pre></div><h2 id="2-multi-head-self-attention-msa">§2 Multi-Head Self Attention (MSA)</h2>
<p>This section is based on Eq. 2.</p>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-appendix-A.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-in-code.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Create a class that inherits from nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">MultiheadSelfAttentionBlock</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#d88200">&#34;&#34;&#34;Creates a multi-head self-attention block (&#34;MSA block&#34; for short).
</span></span></span><span style="display:flex;"><span><span style="color:#d88200">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. Initialize the class with hyperparameters from Table 1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dim</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># Hidden size D from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">num_heads</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">,</span> <span style="color:#75715e"># Heads from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">attn_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">):</span> <span style="color:#75715e"># doesn&#39;t look like the paper uses any dropout in MSABlocks</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. Create the Norm layer (LN)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">layer_norm</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">LayerNorm</span><span style="color:#111">(</span><span style="color:#111">normalized_shape</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 4. Create the Multi-Head Attention (MSA) layer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">multihead_attn</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">MultiheadAttention</span><span style="color:#111">(</span><span style="color:#111">embed_dim</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                    <span style="color:#111">num_heads</span><span style="color:#f92672">=</span><span style="color:#111">num_heads</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                    <span style="color:#111">dropout</span><span style="color:#f92672">=</span><span style="color:#111">attn_dropout</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                    <span style="color:#111">batch_first</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e"># does our batch dimension come first?</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. Create a forward() method to pass the data throguh the layers</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">layer_norm</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">attn_output</span><span style="color:#111">,</span> <span style="color:#111">_</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">multihead_attn</span><span style="color:#111">(</span><span style="color:#111">query</span><span style="color:#f92672">=</span><span style="color:#111">x</span><span style="color:#111">,</span> <span style="color:#75715e"># query embeddings </span>
</span></span><span style="display:flex;"><span>                                             <span style="color:#111">key</span><span style="color:#f92672">=</span><span style="color:#111">x</span><span style="color:#111">,</span> <span style="color:#75715e"># key embeddings</span>
</span></span><span style="display:flex;"><span>                                             <span style="color:#111">value</span><span style="color:#f92672">=</span><span style="color:#111">x</span><span style="color:#111">,</span> <span style="color:#75715e"># value embeddings</span>
</span></span><span style="display:flex;"><span>                                             <span style="color:#111">need_weights</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span> <span style="color:#75715e"># do we need the weights or just the layer outputs?</span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">attn_output</span>
</span></span></code></pre></div><p>To use <code>MultiheadSelfAttentionBlock</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create an instance of MSABlock, embedding_dim and num_heads are from Table 1</span>
</span></span><span style="display:flex;"><span><span style="color:#111">multihead_self_attention_block</span> <span style="color:#f92672">=</span> <span style="color:#111">MultiheadSelfAttentionBlock</span><span style="color:#111">(</span><span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> 
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">num_heads</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">)</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Pass patch and position image embedding through MSABlock</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patched_image_through_msa_block</span> <span style="color:#f92672">=</span> <span style="color:#111">multihead_self_attention_block</span><span style="color:#111">(</span><span style="color:#111">patch_and_position_embedding</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># or</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># patched_image_through_msa_block = multihead_self_attention_block(torch.rand(1, 197, 768))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Input shape of MSA block: </span><span style="color:#d88200">{</span><span style="color:#111">patch_and_position_embedding</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Output shape MSA block: </span><span style="color:#d88200">{</span><span style="color:#111">patched_image_through_msa_block</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>will get:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Input shape of MSA block: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Output shape MSA block: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span></code></pre></div><p>Note that input and output have the same shape, which is common for different kinds of <code>AttentionBlock</code>.</p>
<h2 id="3-multilayer-perception-mlp">§3 Multilayer Perception (MLP)</h2>
<p>This section is based on Eq. 3.</p>
<p><img
  src="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-equation-3-annotated.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-3-mapped-to-code.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Create a class that inherits from nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">MLPBlock</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#d88200">&#34;&#34;&#34;Creates a layer normalized multilayer perceptron block (&#34;MLP block&#34; for short).&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dim</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># Hidden Size D from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">mlp_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3072</span><span style="color:#111">,</span> <span style="color:#75715e"># MLP size from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">):</span> <span style="color:#75715e"># Dropout from Table 3 for ViT-Base</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. Create the Norm layer (LN)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">layer_norm</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">LayerNorm</span><span style="color:#111">(</span><span style="color:#111">normalized_shape</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 4. Create the Multilayer perceptron (MLP) layer(s)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">mlp</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Sequential</span><span style="color:#111">(</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">in_features</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#111">out_features</span><span style="color:#f92672">=</span><span style="color:#111">mlp_size</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">GELU</span><span style="color:#111">(),</span> <span style="color:#75715e"># &#34;The MLP contains two layers with a GELU non-linearity (section 3.1).&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Dropout</span><span style="color:#111">(</span><span style="color:#111">p</span><span style="color:#f92672">=</span><span style="color:#111">dropout</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">in_features</span><span style="color:#f92672">=</span><span style="color:#111">mlp_size</span><span style="color:#111">,</span> <span style="color:#75715e"># needs to take same in_features as out_features of layer above</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#111">out_features</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">),</span> <span style="color:#75715e"># take back to embedding_dim</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Dropout</span><span style="color:#111">(</span><span style="color:#111">p</span><span style="color:#f92672">=</span><span style="color:#111">dropout</span><span style="color:#111">)</span> <span style="color:#75715e"># &#34;Dropout, when used, is applied after every dense layer..&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. Create a forward() method to pass the data throguh the layers</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">layer_norm</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">mlp</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">x</span>
</span></span></code></pre></div><p>To use <code>MLPBlock</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create an instance of MLPBlock</span>
</span></span><span style="display:flex;"><span><span style="color:#111">mlp_block</span> <span style="color:#f92672">=</span> <span style="color:#111">MLPBlock</span><span style="color:#111">(</span><span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># from Table 1 </span>
</span></span><span style="display:flex;"><span>                     <span style="color:#111">mlp_size</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3072</span><span style="color:#111">,</span> <span style="color:#75715e"># from Table 1</span>
</span></span><span style="display:flex;"><span>                     <span style="color:#111">dropout</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">)</span> <span style="color:#75715e"># from Table 3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Pass output of MSABlock through MLPBlock</span>
</span></span><span style="display:flex;"><span><span style="color:#111">patched_image_through_mlp_block</span> <span style="color:#f92672">=</span> <span style="color:#111">mlp_block</span><span style="color:#111">(</span><span style="color:#111">patched_image_through_msa_block</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># or</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># patched_image_through_msa_block = mlp_block(torch.rand(1, 197, 768))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Input shape of MLP block: </span><span style="color:#d88200">{</span><span style="color:#111">patched_image_through_mlp_block</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Output shape MLP block: </span><span style="color:#d88200">{</span><span style="color:#111">patched_image_through_mlp_block</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>will get:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Input shape of MLP block: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span><span style="display:flex;"><span>Output shape MLP block: torch.Size<span style="color:#f92672">([</span>1, 197, 768<span style="color:#f92672">])</span>
</span></span></code></pre></div><p>Note that, again, input and output have the same shape.</p>
<h2 id="4-transformer-encoder">§4 Transformer Encoder</h2>
<p><img
  src="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-transformer-encoder-highlighted.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>


<img
  src="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-transformer-encoder-mapped-to-code.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<h3 id="41-put-blocks-above-together">§4.1 Put Blocks Above Together</h3>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Create a class that inherits from nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">TransformerEncoderBlock</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#d88200">&#34;&#34;&#34;Creates a Transformer Encoder block.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dim</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># Hidden size D from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">num_heads</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">,</span> <span style="color:#75715e"># Heads from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">mlp_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3072</span><span style="color:#111">,</span> <span style="color:#75715e"># MLP size from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">mlp_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">,</span> <span style="color:#75715e"># Amount of dropout for dense layers from Table 3 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">attn_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">):</span> <span style="color:#75715e"># Amount of dropout for attention layers</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. Create MSA block (equation 2)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">msa_block</span> <span style="color:#f92672">=</span> <span style="color:#111">MultiheadSelfAttentionBlock</span><span style="color:#111">(</span><span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                     <span style="color:#111">num_heads</span><span style="color:#f92672">=</span><span style="color:#111">num_heads</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                     <span style="color:#111">attn_dropout</span><span style="color:#f92672">=</span><span style="color:#111">attn_dropout</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 4. Create MLP block (equation 3)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">mlp_block</span> <span style="color:#f92672">=</span>  <span style="color:#111">MLPBlock</span><span style="color:#111">(</span><span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                   <span style="color:#111">mlp_size</span><span style="color:#f92672">=</span><span style="color:#111">mlp_size</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                   <span style="color:#111">dropout</span><span style="color:#f92672">=</span><span style="color:#111">mlp_dropout</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. Create a forward() method  </span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 6. Create residual connection for MSA block (add the input to the output)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span>  <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">msa_block</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span> <span style="color:#f92672">+</span> <span style="color:#111">x</span> 
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 7. Create residual connection for MLP block (add the input to the output)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">mlp_block</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span> <span style="color:#f92672">+</span> <span style="color:#111">x</span> 
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">x</span>
</span></span></code></pre></div><p>Use <code>torchinfo.summary</code> to get the information about <code>transformer_encoder_block</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create an instance of TransformerEncoderBlock</span>
</span></span><span style="display:flex;"><span><span style="color:#111">transformer_encoder_block</span> <span style="color:#f92672">=</span> <span style="color:#111">TransformerEncoderBlock</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print an input and output summary of our Transformer Encoder (uncomment for full output)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">summary</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#f92672">=</span><span style="color:#111">transformer_encoder_block</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">input_size</span><span style="color:#f92672">=</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">197</span><span style="color:#111">,</span> <span style="color:#ae81ff">768</span><span style="color:#111">),</span> <span style="color:#75715e"># (batch_size, num_patches, embedding_dimension)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_names</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;input_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;output_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;num_params&#34;</span><span style="color:#111">],</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_width</span><span style="color:#f92672">=</span><span style="color:#ae81ff">20</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">row_settings</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;var_names&#34;</span><span style="color:#111">])</span>
</span></span></code></pre></div><p>will get:</p>
<p><img
  src="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-summary-output-transformer-encoder.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<h3 id="42-build-encoder-with-torchnntransformerencoderlayerhttpspytorchorgdocsstablegeneratedtorchnntransformerencoderlayerhtmltorchnntransformerencoderlayer">§4.2 Build Encoder with <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer"><code>torch.nn.TransformerEncoderLayer()</code></a>:</h3>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create the same as above with torch.nn.TransformerEncoderLayer()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch_transformer_encoder_layer</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">TransformerEncoderLayer</span><span style="color:#111">(</span><span style="color:#111">d_model</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># Hidden size D from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">nhead</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">,</span> <span style="color:#75715e"># Heads from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">dim_feedforward</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3072</span><span style="color:#111">,</span> <span style="color:#75715e"># MLP size from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">dropout</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">,</span> <span style="color:#75715e"># Amount of dropout for dense layers from Table 3 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">activation</span><span style="color:#f92672">=</span><span style="color:#d88200">&#34;gelu&#34;</span><span style="color:#111">,</span> <span style="color:#75715e"># GELU non-linear activation</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">batch_first</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">,</span> <span style="color:#75715e"># Do our batches come first?</span>
</span></span><span style="display:flex;"><span>                                                             <span style="color:#111">norm_first</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e"># Normalize first or after MSA/MLP layers?</span>
</span></span></code></pre></div><p>Use <code>torchinfo.summary</code> to get the information about <code>torch_transformer_encoder_layer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Get the output of PyTorch&#39;s version of the Transformer Encoder (uncomment for full output)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">summary</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#f92672">=</span><span style="color:#111">torch_transformer_encoder_layer</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">input_size</span><span style="color:#f92672">=</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">197</span><span style="color:#111">,</span> <span style="color:#ae81ff">768</span><span style="color:#111">),</span> <span style="color:#75715e"># (batch_size, num_patches, embedding_dimension)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_names</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;input_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;output_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;num_params&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;trainable&#34;</span><span style="color:#111">],</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_width</span><span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">row_settings</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;var_names&#34;</span><span style="color:#111">])</span>
</span></span></code></pre></div><p>will get:</p>
<p><img
  src="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-summary-output-pytorch-transformer-encoder.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<p>Using <code>torch.nn</code> will be slightly different, they have $\mathrm{3}$ advantages in general:</p>
<ul>
<li>Faster.</li>
<li>More robust.</li>
<li>Easier to build.</li>
</ul>
<h2 id="5-vit">§5 ViT</h2>
<p>Eq. 4 is the final <code>torch.nn.LayerNorm()</code> and <code>torch.nn.Linear</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 1. Create a ViT class that inherits from nn.Module</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">ViT</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#d88200">&#34;&#34;&#34;Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">img_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">224</span><span style="color:#111">,</span> <span style="color:#75715e"># Training resolution from Table 3 in ViT paper</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">in_channels</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#75715e"># Number of channels in input image</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">patch_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#75715e"># Patch size</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">num_transformer_layers</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">,</span> <span style="color:#75715e"># Layers from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dim</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">768</span><span style="color:#111">,</span> <span style="color:#75715e"># Hidden size D from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">mlp_size</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3072</span><span style="color:#111">,</span> <span style="color:#75715e"># MLP size from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">num_heads</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#111">,</span> <span style="color:#75715e"># Heads from Table 1 for ViT-Base</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">attn_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#75715e"># Dropout for attention projection</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">mlp_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">,</span> <span style="color:#75715e"># Dropout for dense/MLP layers </span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">embedding_dropout</span><span style="color:#111">:</span><span style="color:#111">float</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span><span style="color:#111">,</span> <span style="color:#75715e"># Dropout for patch and position embeddings</span>
</span></span><span style="display:flex;"><span>                 <span style="color:#111">num_classes</span><span style="color:#111">:</span><span style="color:#111">int</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span><span style="color:#111">):</span> <span style="color:#75715e"># Default for ImageNet but can customize this</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span> <span style="color:#75715e"># don&#39;t forget the super().__init__()!</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 3. Make the image size is divisble by the patch size </span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">assert</span> <span style="color:#111">img_size</span> <span style="color:#f92672">%</span> <span style="color:#111">patch_size</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#d88200">f</span><span style="color:#d88200">&#34;Image size must be divisible by patch size, image size: </span><span style="color:#d88200">{</span><span style="color:#111">img_size</span><span style="color:#d88200">}</span><span style="color:#d88200">, patch size: </span><span style="color:#d88200">{</span><span style="color:#111">patch_size</span><span style="color:#d88200">}</span><span style="color:#d88200">.&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 4. Calculate number of patches (height * width/patch^2)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">num_patches</span> <span style="color:#f92672">=</span> <span style="color:#111">(</span><span style="color:#111">img_size</span> <span style="color:#f92672">*</span> <span style="color:#111">img_size</span><span style="color:#111">)</span> <span style="color:#f92672">//</span> <span style="color:#111">patch_size</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>                 
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">class_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Parameter</span><span style="color:#111">(</span><span style="color:#111">data</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">embedding_dim</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>                                            <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 6. Create learnable position embedding</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">position_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Parameter</span><span style="color:#111">(</span><span style="color:#111">data</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">num_patches</span><span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">embedding_dim</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>                                               <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 7. Create embedding dropout value</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">embedding_dropout</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Dropout</span><span style="color:#111">(</span><span style="color:#111">p</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dropout</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 8. Create patch embedding layer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">patch_embedding</span> <span style="color:#f92672">=</span> <span style="color:#111">PatchEmbedding</span><span style="color:#111">(</span><span style="color:#111">in_channels</span><span style="color:#f92672">=</span><span style="color:#111">in_channels</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                              <span style="color:#111">patch_size</span><span style="color:#f92672">=</span><span style="color:#111">patch_size</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                              <span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) </span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Note: The &#34;*&#34; means &#34;all&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">transformer_encoder</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Sequential</span><span style="color:#111">(</span><span style="color:#f92672">*</span><span style="color:#111">[</span><span style="color:#111">TransformerEncoderBlock</span><span style="color:#111">(</span><span style="color:#111">embedding_dim</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                                            <span style="color:#111">num_heads</span><span style="color:#f92672">=</span><span style="color:#111">num_heads</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                                            <span style="color:#111">mlp_size</span><span style="color:#f92672">=</span><span style="color:#111">mlp_size</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>                                                                            <span style="color:#111">mlp_dropout</span><span style="color:#f92672">=</span><span style="color:#111">mlp_dropout</span><span style="color:#111">)</span> <span style="color:#00a8c8">for</span> <span style="color:#111">_</span> <span style="color:#f92672">in</span> <span style="color:#111">range</span><span style="color:#111">(</span><span style="color:#111">num_transformer_layers</span><span style="color:#111">)])</span>
</span></span><span style="display:flex;"><span>       
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 10. Create classifier head</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">classifier</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Sequential</span><span style="color:#111">(</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">LayerNorm</span><span style="color:#111">(</span><span style="color:#111">normalized_shape</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">in_features</span><span style="color:#f92672">=</span><span style="color:#111">embedding_dim</span><span style="color:#111">,</span> 
</span></span><span style="display:flex;"><span>                      <span style="color:#111">out_features</span><span style="color:#f92672">=</span><span style="color:#111">num_classes</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 11. Create a forward() method</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 12. Get batch size</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">batch_size</span> <span style="color:#f92672">=</span> <span style="color:#111">x</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#111">]</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 13. Create class token embedding and expand it to match the batch size (equation 1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">class_token</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">class_embedding</span><span style="color:#f92672">.</span><span style="color:#111">expand</span><span style="color:#111">(</span><span style="color:#111">batch_size</span><span style="color:#111">,</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span> <span style="color:#75715e"># &#34;-1&#34; means to infer the dimension (try this line on its own)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 14. Create patch embedding (equation 1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">patch_embedding</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 15. Concat class embedding and patch embedding (equation 1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">cat</span><span style="color:#111">((</span><span style="color:#111">class_token</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">),</span> <span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 16. Add position embedding to patch embedding (equation 1) </span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">position_embedding</span> <span style="color:#f92672">+</span> <span style="color:#111">x</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 17. Run embedding dropout (Appendix B.1)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">embedding_dropout</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">transformer_encoder</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 19. Put 0 index logit through classifier (equation 4)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">classifier</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">[:,</span> <span style="color:#ae81ff">0</span><span style="color:#111">])</span> <span style="color:#75715e"># run on each sample in a batch at 0 index</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">x</span>       
</span></span></code></pre></div><p>Use <code>torchinfo.summary</code> to get the information about <code>torch_transformer_encoder_layer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create a random tensor with same shape as a single image</span>
</span></span><span style="display:flex;"><span><span style="color:#111">random_image_tensor</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">)</span> <span style="color:#75715e"># (batch_size, color_channels, height, width)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create an instance of ViT with the number of classes we&#39;re working with (pizza, steak, sushi)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># vit = ViT(num_classes=len(class_names))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">vit</span> <span style="color:#f92672">=</span> <span style="color:#111">ViT</span><span style="color:#111">(</span><span style="color:#111">num_classes</span><span style="color:#f92672">=</span><span style="color:#ae81ff">3</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print a summary of our custom ViT model using torchinfo (uncomment for actual output)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">summary</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#f92672">=</span><span style="color:#111">vit</span><span style="color:#111">,</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#111">input_size</span><span style="color:#f92672">=</span><span style="color:#111">(</span><span style="color:#ae81ff">32</span><span style="color:#111">,</span> <span style="color:#ae81ff">3</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">,</span> <span style="color:#ae81ff">224</span><span style="color:#111">),</span> <span style="color:#75715e"># (batch_size, color_channels, height, width)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># col_names=[&#34;input_size&#34;], # uncomment for smaller output</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_names</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;input_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;output_size&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;num_params&#34;</span><span style="color:#111">,</span> <span style="color:#d88200">&#34;trainable&#34;</span><span style="color:#111">],</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">col_width</span><span style="color:#f92672">=</span><span style="color:#ae81ff">20</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">row_settings</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#d88200">&#34;var_names&#34;</span><span style="color:#111">]</span>
</span></span><span style="display:flex;"><span><span style="color:#111">)</span>
</span></span></code></pre></div><p>will get:</p>
<p><img
  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-summary-output-custom-vit-model.png"
  alt=""
  loading="lazy"
  decoding="async"
  class="full-width"
/>

</p>
<h2 id="6-optimizer-loss-function">§6 Optimizer, Loss Function</h2>
<p>Generally these two will be different in different model, I&rsquo;ll write about them later.</p>
<p>In ViT they are <code>torch.optim.Adam()</code> and <code>torch.nn.CrossEntropyLoss()</code>, see <a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#9-setting-up-training-code-for-our-vit-model">Section 9. Setting up training code for our ViT model</a>.</p>
<h2 id="7-use-pretrained-model-transfer-learning">§7 Use Pretrained model (Transfer Learning)</h2>
<p>See <a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset">Section 10. Using a pretrained ViT from <code>torchvision.models</code> on the same dataset</a>.</p>
<p>For more information on Transfer Learning, see <a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">06. PyTorch Transfer Learning</a>. Freeze part of the model and train the rest of the model is fun. I will get to this some day, but since in physics we are dealing with quite different dataset than Natural Language Processing (NLP) or Computer Vision (CV), it&rsquo;s probably not gonna be really helpful. Except classification for Galaxies or Gravitational Wave, which are close to traditional CV classification missions.</p>
<h2 id="8-cnn-or-transformer">§8 CNN or Transformer</h2>
<p>The comparison of CNN and Transformer in <a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size">Section 10.6 Save feature extractor ViT model and check file size</a> is interesting.</p>
<p>But transformer is better than CNN in $\mathrm{2}$ ways:</p>
<ul>
<li>Multimodal. Embedding then <code>torch.cat</code> is surprisingly useful.</li>
<li>No signs of <a href="https://en.wikipedia.org/wiki/Overfitting#Machine_learning">overfitting</a> yet.</li>
</ul>
<p>At scale, they tend to have same performance, see <a href="https://arxiv.org/abs/2310.16764">[2310.16764] <em>ConvNets Match Vision Transformers at Scale</em></a>.</p>

      
      <div class="post-date">
        <span class="g time">June 24, 2023 </span> &#8729;
         
         <a href="https://chenlinear.github.io/tags/programming/">programming</a>
      </div>
      
    </section>
    
    <div id="comments">
      <script src="https://utteranc.es/client.js"
    repo=chenlinear/chenlinear.github.io
    issue-term="pathname"
    theme=github-light
    crossorigin="anonymous"
    async>
</script>

    </div>
    
  </div>
</main>
</body>
</html>
